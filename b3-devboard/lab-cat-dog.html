
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>19. Lab 4: Transfer Learning with Cats vs. Dogs &#8212; ECE 386</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'b3-devboard/lab-cat-dog';</script>
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="22. Containerization" href="containerization.html" />
    <link rel="prev" title="18. C&amp;C: Edge Inference" href="../b2-edge/cc-edge.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="ECE 386 - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="ECE 386 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    ECE 386: AI Hardware Applications
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Prediction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../b1-prediction/hello-colab.html">1. Hello, Colab!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b1-prediction/networks-tooling.html">2. Networks and Tooling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b1-prediction/prediction-machines.html">3. Prediction Machines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b1-prediction/datasets.html">4. Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b1-prediction/ice-kmeans.html">5. ICE 1: K-Means Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b1-prediction/ice-digits-dnn.html">6. ICE 2: Handwritten Digits - DNN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b1-prediction/cloud-hosting.html">7. Cloud Hosting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b1-prediction/lab-digits-api.html">8. Lab 1: Handwritten Digits - FastAPI</a></li>

<li class="toctree-l1"><a class="reference internal" href="../b1-prediction/cc-prediction.html">10. C&amp;C: Prediction and Dimensionality</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Edge Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../b2-edge/edge-intro.html">11. Edge Inference Intro</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b2-edge/lab-cortex-benchmark.html">12. Lab 2: Cortex DSP Benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b2-edge/cortex-architecture.html">13. ARM Cortex Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b2-edge/quantization.html">14. Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b2-edge/lab-impulse-kws.html">15. Lab 3: Edge Impulse KWS</a></li>

<li class="toctree-l1"><a class="reference internal" href="../b2-edge/memory-management.html">17. Memory Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b2-edge/cc-edge.html">18. C&amp;C: Edge Inference</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Development Boards</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">19. Lab 4: Transfer Learning with Cats vs. Dogs</a></li>


<li class="toctree-l1"><a class="reference internal" href="containerization.html">22. Containerization</a></li>
<li class="toctree-l1"><a class="reference internal" href="ice-whisper.html">23. ICE 3: Whisper Transcription</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-architecture.html">24. GPU Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="ice-gpu-acceleration.html">25. ICE 4: GPU Acceleration</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Large Language Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../b4-llm/text-vectorization.html">26. Text Vectorization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b4-llm/llm.html">27. Large Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b4-llm/lab-prompt-engineering.html">28. Lab 5: Prompt Engineering</a></li>

<li class="toctree-l1"><a class="reference internal" href="../b4-llm/cc-gpu-llm.html">30. C&amp;C GPUs and LLMs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Final Block</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../b5-final/final-project.html">31. Final Project</a></li>



<li class="toctree-l1"><a class="reference internal" href="../b5-final/command-risk.html">35. Command and Risk</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b5-final/student-choice.html">36. Student Choice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b5-final/alternative-architectures.html">37. Alternative Architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b5-final/supercomputers.html">38. Super Computers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b5-final/wrapup.html">39. Wrap and Critique</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../appendix/flash-edge-impulse.html">Flash Edge Impulse Firmware to Arduino</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix/jetson-setup.html">Jetson Setup</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/usafa-ece/ece386-book/blob/main/book/b3-devboard/lab-cat-dog.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/usafa-ece/ece386-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/usafa-ece/ece386-book/issues/new?title=Issue%20on%20page%20%2Fb3-devboard/lab-cat-dog.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/b3-devboard/lab-cat-dog.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lab 4: Transfer Learning with Cats vs. Dogs</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">19. Lab 4: Transfer Learning with Cats vs. Dogs</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">19.1. Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#background">Background</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#freezing-layers-understanding-the-trainable-attribute">Freezing layers: understanding the <code class="docutils literal notranslate"><span class="pre">trainable</span></code> attribute</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recursive-setting-of-the-trainable-attribute">Recursive setting of the <code class="docutils literal notranslate"><span class="pre">trainable</span></code> attribute</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-typical-transfer-learning-workflow">The typical transfer-learning workflow</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning">Fine-tuning</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-lab">19.2. Pre-lab</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-the-data">Getting the data</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#standardizing-the-data">Standardizing the data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-random-data-augmentation">Using random data augmentation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#build-a-model">Build a model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-the-top-layer">Train the top layer</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-curves">Learning Curves</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-the-entire-model">Fine-tuning the entire model</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#test">Test</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#save-the-model">Save the Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensorflow-lite-conversion">TensorFlow Lite Conversion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#lab">20. Lab</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#litert">20.1. LiteRT</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#signatures">Signatures</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#invoking">Invoking</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#litert-summary">LiteRT Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-webcam">20.2. Using Webcam</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarking">20.3. Benchmarking</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#measuring-cache-performance-with-perf-tool">Measuring cache performance with perf tool</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#live-inference">21. üê± üê∂ Live Inference</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#template">21.1. Template</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-configuration">Optional Configuration</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-code">The code</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deliverables">21.2. Deliverables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#perf-discussion">Perf discussion</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="lab-4-transfer-learning-with-cats-vs-dogs">
<h1><a class="toc-backref" href="#id3" role="doc-backlink"><span class="section-number">19. </span>Lab 4: Transfer Learning with Cats vs. Dogs</a><a class="headerlink" href="#lab-4-transfer-learning-with-cats-vs-dogs" title="Link to this heading">#</a></h1>
<nav class="contents" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#lab-4-transfer-learning-with-cats-vs-dogs" id="id3">Lab 4: Transfer Learning with Cats vs. Dogs</a></p>
<ul>
<li><p><a class="reference internal" href="#overview" id="id4">Overview</a></p></li>
<li><p><a class="reference internal" href="#pre-lab" id="id5">Pre-lab</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#lab" id="id6">Lab</a></p>
<ul>
<li><p><a class="reference internal" href="#litert" id="id7">LiteRT</a></p></li>
<li><p><a class="reference internal" href="#using-webcam" id="id8">Using Webcam</a></p></li>
<li><p><a class="reference internal" href="#benchmarking" id="id9">Benchmarking</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#live-inference" id="id10">üê± üê∂ Live Inference</a></p>
<ul>
<li><p><a class="reference internal" href="#template" id="id11">Template</a></p></li>
<li><p><a class="reference internal" href="#deliverables" id="id12">Deliverables</a></p></li>
</ul>
</li>
</ul>
</nav>
<blockquote>
<div><p>Original, from <a class="reference external" href="https://keras.io/guides/transfer_learning/">Keras Developer Guides: Transfer learning &amp; fine-tuning</a></p>
<p>Which is itself adapted from <a class="reference external" href="https://www.manning.com/books/deep-learning-with-python">Deep Learning with Python</a>
and the 2016 blog post <a class="reference external" href="https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html">‚Äúbuilding powerful image classification models using very little data‚Äù</a>.</p>
<p><em>Author: <a class="reference external" href="https://twitter.com/fchollet">fchollet</a> Date created: 2020/04/15 Last modified: 2023/06/25</em></p>
<p>Adapted for ECE 386 by <a class="reference external" href="https://github.com/byarbrough">byarbrough</a> with additions from <a class="reference external" href="https://www.tensorflow.org/tutorials/images/transfer_learning">TensorFlow Tutorials: Transfer learning and fine-tuning</a>, cao Jan 2025.</p>
</div></blockquote>
<p>The <strong>Overview</strong> and <strong>Prelab</strong> are meant to be read and run in Colab or on the department AI server.
The prelab will produce two models (<code class="docutils literal notranslate"><span class="pre">.tflite</span></code> and <code class="docutils literal notranslate"><span class="pre">.keras</span></code>) <strong>which you need to download and save!</strong></p>
<p>The Lab has lots of hints, but you don‚Äôt need to run them in Colab!
Rather, you should clone <a class="github reference external" href="https://github.com/USAFA-ECE/ece386-lab4.git">USAFA-ECE/ece386-lab4.git</a> and ultimately get it working on your Raspberry Pi.</p>
<section id="overview">
<h2><a class="toc-backref" href="#id4" role="doc-backlink"><span class="section-number">19.1. </span>Overview</a><a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p><strong>Pre-reading video:</strong> <a class="reference external" href="https://youtu.be/mPFq5KMxKVw">Kaggle: Transfer Learning</a> (only first 5 minutes)</p>
<p>For this lab you will use <em>transfer learning</em> to make a general purpose image classification model exceptional at telling the difference between cats vs. dogs!</p>
<p>You will ultimately use this model for live inference with visitors to the classroom!</p>
<section id="background">
<h3>Background<a class="headerlink" href="#background" title="Link to this heading">#</a></h3>
<nav class="contents local" id="id1">
<ul class="simple">
<li><p><a class="reference internal" href="#introduction" id="id13">Introduction</a></p></li>
<li><p><a class="reference internal" href="#freezing-layers-understanding-the-trainable-attribute" id="id14">Freezing layers: understanding the <code class="docutils literal notranslate"><span class="pre">trainable</span></code> attribute</a></p></li>
<li><p><a class="reference internal" href="#recursive-setting-of-the-trainable-attribute" id="id15">Recursive setting of the <code class="docutils literal notranslate"><span class="pre">trainable</span></code> attribute</a></p></li>
<li><p><a class="reference internal" href="#the-typical-transfer-learning-workflow" id="id16">The typical transfer-learning workflow</a></p></li>
<li><p><a class="reference internal" href="#fine-tuning" id="id17">Fine-tuning</a></p></li>
</ul>
</nav>
<section id="introduction">
<h4><a class="toc-backref" href="#id13" role="doc-backlink">Introduction</a><a class="headerlink" href="#introduction" title="Link to this heading">#</a></h4>
<p><strong>Transfer learning</strong> consists of taking features learned on one problem, and
leveraging them on a new, similar problem. For instance, features from a model that has
learned to identify racoons may be useful to kick-start a model meant to identify tanukis.</p>
<p>Transfer learning is usually done for tasks where your dataset has too little data to train a full-scale model from scratch.</p>
<p>The most common incarnation of transfer learning in the context of deep learning is the following workflow:</p>
<ol class="arabic simple">
<li><p>Take layers from a previously trained model.</p></li>
<li><p>Freeze them, so as to avoid destroying any of the information they contain during future training rounds.</p></li>
<li><p>Add some new, trainable layers on top of the frozen layers.
They will learn to turn the old features into predictions on a  new dataset.</p></li>
<li><p>Train the new layers on your dataset.</p></li>
</ol>
<p>A last, optional step, is <strong>fine-tuning</strong>, which consists of unfreezing the entire
model you obtained above (or part of it), and re-training it on the new data with a
very low learning rate. This can potentially achieve meaningful improvements, by
incrementally adapting the pretrained features to the new data.</p>
<p>First, we will go over the Keras <code class="docutils literal notranslate"><span class="pre">trainable</span></code> API in detail, which underlies most
transfer learning &amp; fine-tuning workflows.</p>
</section>
<section id="freezing-layers-understanding-the-trainable-attribute">
<h4><a class="toc-backref" href="#id14" role="doc-backlink">Freezing layers: understanding the <code class="docutils literal notranslate"><span class="pre">trainable</span></code> attribute</a><a class="headerlink" href="#freezing-layers-understanding-the-trainable-attribute" title="Link to this heading">#</a></h4>
<p>Layers &amp; models have three weight attributes:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">weights</span></code> is the list of all weights variables of the layer.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">trainable_weights</span></code> is the list of those that are meant to be updated (via gradient
descent) to minimize the loss during training.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">non_trainable_weights</span></code> is the list of those that aren‚Äôt meant to be trained.
Typically they are updated by the model during the forward pass.</p></li>
</ul>
<p><strong>Example: the <code class="docutils literal notranslate"><span class="pre">Dense</span></code> layer has 2 trainable weights (kernel &amp; bias)</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">layer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">layer</span><span class="o">.</span><span class="n">build</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>  <span class="c1"># Create the weights</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;weights:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weights</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;trainable_weights:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">trainable_weights</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;non_trainable_weights:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">non_trainable_weights</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>weights: 2
trainable_weights: 2
non_trainable_weights: 0
</pre></div>
</div>
<p>In general, all weights are trainable weights. The only built-in layer that has
non-trainable weights is the <code class="docutils literal notranslate"><span class="pre">BatchNormalization</span></code> layer.
It uses non-trainable weights to keep track of the mean and variance of its inputs during training.
To learn how to use non-trainable weights in your own custom layers, see the
<a class="reference internal" href="#/guides/making_new_layers_and_models_via_subclassing/"><span class="xref myst">guide to writing new layers from scratch</span></a>.</p>
<p><strong>Example: the <code class="docutils literal notranslate"><span class="pre">BatchNormalization</span></code> layer has 2 trainable weights and 2 non-trainable weights</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">layer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">()</span>
<span class="n">layer</span><span class="o">.</span><span class="n">build</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>  <span class="c1"># Create the weights</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;weights:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weights</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;trainable_weights:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">trainable_weights</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;non_trainable_weights:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">non_trainable_weights</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>weights: 4
trainable_weights: 2
non_trainable_weights: 2
</pre></div>
</div>
<p>Layers &amp; models also feature a boolean attribute <code class="docutils literal notranslate"><span class="pre">trainable</span></code>. Its value can be changed.
Setting <code class="docutils literal notranslate"><span class="pre">layer.trainable</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code> moves all the layer‚Äôs weights from trainable to
non-trainable.  This is called ‚Äúfreezing‚Äù the layer: the state of a frozen layer won‚Äôt
be updated during training (either when training with <code class="docutils literal notranslate"><span class="pre">fit()</span></code> or when training with
any custom loop that relies on <code class="docutils literal notranslate"><span class="pre">trainable_weights</span></code> to apply gradient updates).</p>
<p><strong>Example: setting <code class="docutils literal notranslate"><span class="pre">trainable</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code></strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">layer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">layer</span><span class="o">.</span><span class="n">build</span><span class="p">((</span><span class="kc">None</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>  <span class="c1"># Create the weights</span>
<span class="n">layer</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># Freeze the layer</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;weights:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weights</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;trainable_weights:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">trainable_weights</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;non_trainable_weights:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">non_trainable_weights</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>weights: 2
trainable_weights: 0
non_trainable_weights: 2
</pre></div>
</div>
<p>When a trainable weight becomes non-trainable, its value is no longer updated during training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make a model with 2 layers</span>
<span class="n">layer1</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
<span class="n">layer2</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,)),</span> <span class="n">layer1</span><span class="p">,</span> <span class="n">layer2</span><span class="p">])</span>

<span class="c1"># Freeze the first layer</span>
<span class="n">layer1</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># Keep a copy of the weights of layer1 for later reference</span>
<span class="n">initial_layer1_weights_values</span> <span class="o">=</span> <span class="n">layer1</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>

<span class="c1"># Train the model</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;adam&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>

<span class="c1"># Check that the weights of layer1 have not changed during training</span>
<span class="n">final_layer1_weights_values</span> <span class="o">=</span> <span class="n">layer1</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>
<span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span>
    <span class="n">initial_layer1_weights_values</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">final_layer1_weights_values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span>
    <span class="n">initial_layer1_weights_values</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">final_layer1_weights_values</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 1s/step - loss: 0.1309
</pre></div>
</div>
<p>Do not confuse the <code class="docutils literal notranslate"><span class="pre">layer.trainable</span></code> attribute with the argument <code class="docutils literal notranslate"><span class="pre">training</span></code> in <code class="docutils literal notranslate"><span class="pre">layer.__call__()</span></code> (which controls whether the layer should run its forward pass in  inference mode or training mode).
For more information, see the <a class="reference external" href="https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute">Keras FAQ</a>.</p>
</section>
<section id="recursive-setting-of-the-trainable-attribute">
<h4><a class="toc-backref" href="#id15" role="doc-backlink">Recursive setting of the <code class="docutils literal notranslate"><span class="pre">trainable</span></code> attribute</a><a class="headerlink" href="#recursive-setting-of-the-trainable-attribute" title="Link to this heading">#</a></h4>
<p>If you set <code class="docutils literal notranslate"><span class="pre">trainable</span> <span class="pre">=</span> <span class="pre">False</span></code> on a model or on any layer that has sublayers,
all children layers become non-trainable as well.</p>
<p><strong>Example:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inner_model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,)),</span>
        <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
        <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,)),</span>
        <span class="n">inner_model</span><span class="p">,</span>
        <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># Freeze the outer model</span>

<span class="k">assert</span> <span class="n">inner_model</span><span class="o">.</span><span class="n">trainable</span> <span class="o">==</span> <span class="kc">False</span>  <span class="c1"># All layers in `model` are now frozen</span>
<span class="k">assert</span> <span class="n">inner_model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">trainable</span> <span class="o">==</span> <span class="kc">False</span>  <span class="c1"># `trainable` is propagated recursively</span>
</pre></div>
</div>
</section>
<section id="the-typical-transfer-learning-workflow">
<h4><a class="toc-backref" href="#id16" role="doc-backlink">The typical transfer-learning workflow</a><a class="headerlink" href="#the-typical-transfer-learning-workflow" title="Link to this heading">#</a></h4>
<p>This leads us to how a typical transfer learning workflow can be implemented in Keras:</p>
<ol class="arabic simple">
<li><p>Instantiate a base model and load pre-trained weights into it.</p></li>
<li><p>Freeze all layers in the base model by setting <code class="docutils literal notranslate"><span class="pre">trainable</span> <span class="pre">=</span> <span class="pre">False</span></code>.</p></li>
<li><p>Create a new model on top of the output of one (or several) layers from the base
model.</p></li>
<li><p>Train your new model on your new dataset.</p></li>
</ol>
<p>Note that an alternative, more lightweight workflow could also be:</p>
<ol class="arabic simple">
<li><p>Instantiate a base model and load pre-trained weights into it.</p></li>
<li><p>Run your new dataset through it and record the output of one (or several) layers
from the base model. This is called <strong>feature extraction</strong>.</p></li>
<li><p>Use that output as input data for a new, smaller model.</p></li>
</ol>
<p>A key advantage of that second workflow is that you only run the base model once on
your data, rather than once per epoch of training. So it‚Äôs a lot faster &amp; cheaper.</p>
<p>An issue with that second workflow, though, is that it doesn‚Äôt allow you to dynamically
modify the input data of your new model during training, which is required when doing
data augmentation, for instance. Transfer learning is typically used for tasks when
your new dataset has too little data to train a full-scale model from scratch, and in
such scenarios data augmentation is very important. So in what follows, we will focus
on the first workflow.</p>
<p>Here‚Äôs what the first workflow looks like in Keras:</p>
<p>First, instantiate a base model with pre-trained weights.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">base_model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">applications</span><span class="o">.</span><span class="n">Xception</span><span class="p">(</span>
    <span class="n">weights</span><span class="o">=</span><span class="s1">&#39;imagenet&#39;</span><span class="p">,</span>  <span class="c1"># Load weights pre-trained on ImageNet.</span>
    <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
    <span class="n">include_top</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Do not include the ImageNet classifier at the top.</span>
</pre></div>
</div>
<p>Then, freeze the base model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">base_model</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
<p>Create a new model on top.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="c1"># We make sure that the base_model is running in inference mode here,</span>
<span class="c1"># by passing `training=False`. This is important for fine-tuning, as you will</span>
<span class="c1"># learn in a few paragraphs.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">base_model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># Convert features of shape `base_model.output_shape[1:]` to vectors</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">GlobalAveragePooling2D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># A Dense classifier with a single unit (binary classification)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
</pre></div>
</div>
<p>Train the model on new data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(),</span>
              <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">BinaryAccuracy</span><span class="p">()])</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">new_dataset</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=...</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=...</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="fine-tuning">
<h4><a class="toc-backref" href="#id17" role="doc-backlink">Fine-tuning</a><a class="headerlink" href="#fine-tuning" title="Link to this heading">#</a></h4>
<p>Once your model has converged on the new data, you can try to unfreeze all or part of
the base model and retrain the whole model end-to-end with a very low learning rate.</p>
<p>This is an optional last step that can potentially give you incremental improvements.
It could also potentially lead to quick overfitting ‚Äì keep that in mind.</p>
<p>It is critical to only do this step <em>after</em> the model with frozen layers has been
trained to convergence. If you mix randomly-initialized trainable layers with
trainable layers that hold pre-trained features, the randomly-initialized layers will
cause very large gradient updates during training, which will destroy your pre-trained
features.</p>
<p>It‚Äôs also critical to use a very low learning rate at this stage, because
you are training a much larger model than in the first round of training, on a dataset
that is typically very small.
As a result, you are at risk of overfitting very quickly if you apply large weight
updates. Here, you only want to readapt the pretrained weights in an incremental way.</p>
<p>This is how to implement fine-tuning of the whole base model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Unfreeze the base model</span>
<span class="n">base_model</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="kc">True</span>

<span class="c1"># It&#39;s important to recompile your model after you make any changes</span>
<span class="c1"># to the `trainable` attribute of any inner layer, so that your changes</span>
<span class="c1"># are take into account</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">),</span>  <span class="c1"># Very low learning rate</span>
              <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">BinaryAccuracy</span><span class="p">()])</span>

<span class="c1"># Train end-to-end. Be careful to stop before you overfit!</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">new_dataset</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=...</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=...</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Important note about <code class="docutils literal notranslate"><span class="pre">compile()</span></code> and <code class="docutils literal notranslate"><span class="pre">trainable</span></code></strong></p>
<p>Calling <code class="docutils literal notranslate"><span class="pre">compile()</span></code> on a model is meant to ‚Äúfreeze‚Äù the behavior of that model. This
implies that the <code class="docutils literal notranslate"><span class="pre">trainable</span></code>
attribute values at the time the model is compiled should be preserved throughout the
lifetime of that model,
until <code class="docutils literal notranslate"><span class="pre">compile</span></code> is called again. Hence, if you change any <code class="docutils literal notranslate"><span class="pre">trainable</span></code> value, make sure
to call <code class="docutils literal notranslate"><span class="pre">compile()</span></code> again on your
model for your changes to be taken into account.</p>
<p><strong>Important notes about <code class="docutils literal notranslate"><span class="pre">BatchNormalization</span></code> layer</strong></p>
<p>Many image models contain <code class="docutils literal notranslate"><span class="pre">BatchNormalization</span></code> layers. That layer is a special case on
every imaginable count. Here are a few things to keep in mind.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">BatchNormalization</span></code> contains 2 non-trainable weights that get updated during
training. These are the variables tracking the mean and variance of the inputs.</p></li>
<li><p>When you set <code class="docutils literal notranslate"><span class="pre">bn_layer.trainable</span> <span class="pre">=</span> <span class="pre">False</span></code>, the <code class="docutils literal notranslate"><span class="pre">BatchNormalization</span></code> layer will run in inference mode, and will not update its mean &amp; variance statistics. This is not the case for other layers in general, as <a class="reference external" href="https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute">weight trainability &amp; inference/training modes are two orthogonal concepts</a>. But the two are tied in the case of the <code class="docutils literal notranslate"><span class="pre">BatchNormalization</span></code> layer.</p></li>
<li><p>When you unfreeze a model that contains <code class="docutils literal notranslate"><span class="pre">BatchNormalization</span></code> layers in order to do fine-tuning, you should keep the <code class="docutils literal notranslate"><span class="pre">BatchNormalization</span></code> layers in inference mode by passing <code class="docutils literal notranslate"><span class="pre">training=False</span></code> when calling the base model. Otherwise the updates applied to the non-trainable weights will suddenly destroy what the model has learned.</p></li>
</ul>
</section>
</section>
</section>
<hr class="docutils" />
<section id="pre-lab">
<h2><a class="toc-backref" href="#id5" role="doc-backlink"><span class="section-number">19.2. </span>Pre-lab</a><a class="headerlink" href="#pre-lab" title="Link to this heading">#</a></h2>
<nav class="contents local" id="id2">
<ul class="simple">
<li><p><a class="reference internal" href="#getting-the-data" id="id18">Getting the data</a></p>
<ul>
<li><p><a class="reference internal" href="#standardizing-the-data" id="id19">Standardizing the data</a></p></li>
<li><p><a class="reference internal" href="#using-random-data-augmentation" id="id20">Using random data augmentation</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#build-a-model" id="id21">Build a model</a></p></li>
<li><p><a class="reference internal" href="#train-the-top-layer" id="id22">Train the top layer</a></p>
<ul>
<li><p><a class="reference internal" href="#learning-curves" id="id23">Learning Curves</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#fine-tuning-the-entire-model" id="id24">Fine-tuning the entire model</a></p>
<ul>
<li><p><a class="reference internal" href="#test" id="id25">Test</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#save-the-model" id="id26">Save the Model</a></p></li>
<li><p><a class="reference internal" href="#tensorflow-lite-conversion" id="id27">TensorFlow Lite Conversion</a></p></li>
</ul>
</nav>
<p>For the pre-lab, follow this end-to-end example: fine-tuning an image classification model on a cats vs. dogs dataset</p>
<ol class="arabic simple">
<li><p>Watch the pre-reading video.</p></li>
<li><p>Load and explore the dataset.</p></li>
<li><p>Load the MobileNet model.</p></li>
<li><p>Perform transfer learning on the model.</p></li>
<li><p>Save a copy of the model for use in the lab.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">keras</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">keras</span><span class="w"> </span><span class="kn">import</span> <span class="n">layers</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tf</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tensorflow_datasets</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">tfds</span>

<span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">2016</span><span class="p">)</span>  <span class="c1"># For reproducability</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Running Tensorflow version&quot;</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>

<span class="n">device_name</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">gpu_device_name</span><span class="p">()</span>
<span class="k">if</span> <span class="n">device_name</span> <span class="o">!=</span> <span class="s2">&quot;/device:GPU:0&quot;</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">SystemError</span><span class="p">(</span><span class="s2">&quot;GPU device not found&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Found GPU at: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">device_name</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<section id="getting-the-data">
<h3><a class="toc-backref" href="#id18" role="doc-backlink">Getting the data</a><a class="headerlink" href="#getting-the-data" title="Link to this heading">#</a></h3>
<p>First, let‚Äôs fetch the cats vs. dogs dataset using TFDS. If you have your own dataset,
you‚Äôll probably want to use the utility
<code class="docutils literal notranslate"><span class="pre">keras.utils.image_dataset_from_directory</span></code> to generate similar labeled
dataset objects from a set of images on disk filed into class-specific folders.</p>
<p>Transfer learning is most useful when working with very small datasets. To keep our
dataset small, we will use 40% of the original training data (25,000 images) for
training, 10% for validation, and 10% for testing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tfds</span><span class="o">.</span><span class="n">disable_progress_bar</span><span class="p">()</span>

<span class="n">train_ds</span><span class="p">,</span> <span class="n">validation_ds</span><span class="p">,</span> <span class="n">test_ds</span> <span class="o">=</span> <span class="n">tfds</span><span class="o">.</span><span class="n">load</span><span class="p">(</span>
    <span class="s2">&quot;cats_vs_dogs&quot;</span><span class="p">,</span>
    <span class="c1"># Reserve 10% for validation and 10% for test</span>
    <span class="n">split</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;train[:40%]&quot;</span><span class="p">,</span> <span class="s2">&quot;train[40%:50%]&quot;</span><span class="p">,</span> <span class="s2">&quot;train[50%:60%]&quot;</span><span class="p">],</span>
    <span class="n">as_supervised</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Include labels</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of training samples: </span><span class="si">{</span><span class="n">train_ds</span><span class="o">.</span><span class="n">cardinality</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of validation samples: </span><span class="si">{</span><span class="n">validation_ds</span><span class="o">.</span><span class="n">cardinality</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of test samples: </span><span class="si">{</span><span class="n">test_ds</span><span class="o">.</span><span class="n">cardinality</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dtype of training samles: </span><span class="si">{</span><span class="n">train_ds</span><span class="o">.</span><span class="n">element_spec</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dtype of training labels: </span><span class="si">{</span><span class="n">train_ds</span><span class="o">.</span><span class="n">element_spec</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>These are the first 9 images in the training dataset ‚Äì as you can see, they‚Äôre all
different sizes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_ds</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">9</span><span class="p">)):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">label</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can also see that label 1 is ‚Äúdog‚Äù and label 0 is ‚Äúcat‚Äù.</p>
<section id="standardizing-the-data">
<h4><a class="toc-backref" href="#id19" role="doc-backlink">Standardizing the data</a><a class="headerlink" href="#standardizing-the-data" title="Link to this heading">#</a></h4>
<p>Our raw images have a variety of sizes. In addition, each pixel consists of 3 integer
values between 0 and 255 (RGB level values). This isn‚Äôt a great fit for feeding a
neural network. We need to do 2 things:</p>
<ul class="simple">
<li><p>Standardize to a fixed image size. We pick 150x150.</p></li>
<li><p>Normalize pixel values to what the model expects. We‚Äôll do this using a layer within the model itself.</p></li>
</ul>
<p>In general, it‚Äôs a good practice to develop models that take raw data as input, as
opposed to models that take already-preprocessed data. The reason being that, if your
model expects preprocessed data, any time you export your model to use it elsewhere
(in a web browser, in a mobile app), you‚Äôll need to reimplement the exact same
preprocessing pipeline. This gets very tricky very quickly. So we should do the least
possible amount of preprocessing before hitting the model.</p>
<p>Here, we‚Äôll do image resizing in the data pipeline (because a deep neural network can
only process contiguous batches of data), and we‚Äôll do the input value scaling as part
of the model, when we create it.</p>
<p>Let‚Äôs resize images to 150x150:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">resize_fn</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Resizing</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>

<span class="n">train_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="n">resize_fn</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">))</span>
<span class="n">validation_ds</span> <span class="o">=</span> <span class="n">validation_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="n">resize_fn</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">))</span>
<span class="n">test_ds</span> <span class="o">=</span> <span class="n">test_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="n">resize_fn</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training set </span><span class="si">{</span><span class="n">train_ds</span><span class="o">.</span><span class="n">element_spec</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="using-random-data-augmentation">
<h4><a class="toc-backref" href="#id20" role="doc-backlink">Using random data augmentation</a><a class="headerlink" href="#using-random-data-augmentation" title="Link to this heading">#</a></h4>
<p>When you don‚Äôt have a large image dataset, it‚Äôs a good practice to artificially
introduce sample diversity by applying random yet realistic transformations to
the training images, such as random horizontal flipping or small random rotations. This
helps expose the model to different aspects of the training data while slowing down
overfitting.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Cache loads the dataset into memory</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span>

<span class="n">augmentation_layers</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">RandomFlip</span><span class="p">(</span><span class="s2">&quot;horizontal&quot;</span><span class="p">),</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">RandomRotation</span><span class="p">(</span><span class="mf">0.1</span><span class="p">),</span>
<span class="p">]</span>


<span class="k">def</span><span class="w"> </span><span class="nf">data_augmentation</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">augmentation_layers</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="c1"># This map means data_augmentation will be called when the dataset is iterated</span>
<span class="n">train_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="n">data_augmentation</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Let‚Äôs batch the data and use prefetching to optimize loading speed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="kn">import</span> <span class="n">data</span> <span class="k">as</span> <span class="n">tf_data</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="n">train_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf_data</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>
<span class="n">validation_ds</span> <span class="o">=</span> <span class="n">validation_ds</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf_data</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span>
<span class="n">test_ds</span> <span class="o">=</span> <span class="n">test_ds</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">tf_data</span><span class="o">.</span><span class="n">AUTOTUNE</span><span class="p">)</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of training batches: </span><span class="si">{</span><span class="n">train_ds</span><span class="o">.</span><span class="n">cardinality</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of validation batches: </span><span class="si">{</span><span class="n">validation_ds</span><span class="o">.</span><span class="n">cardinality</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of test batches: </span><span class="si">{</span><span class="n">test_ds</span><span class="o">.</span><span class="n">cardinality</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let‚Äôs visualize what the first image of the first batch looks like after various random
transformations:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">first_image</span> <span class="o">=</span> <span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">augmented_image</span> <span class="o">=</span> <span class="n">data_augmentation</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">first_image</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">augmented_image</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;int32&quot;</span><span class="p">))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="build-a-model">
<h3><a class="toc-backref" href="#id21" role="doc-backlink">Build a model</a><a class="headerlink" href="#build-a-model" title="Link to this heading">#</a></h3>
<p>Now let‚Äôs built a model that follows the blueprint we‚Äôve explained earlier.</p>
<p>Note that:</p>
<ul class="simple">
<li><p>We add a <code class="docutils literal notranslate"><span class="pre">Rescaling</span></code> layer to scale input values (initially in the <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">255]</span></code>
range) to the <code class="docutils literal notranslate"><span class="pre">[-1,</span> <span class="pre">1]</span></code> range.</p></li>
<li><p>We add a <code class="docutils literal notranslate"><span class="pre">Dropout</span></code> layer before the classification layer, for regularization.</p></li>
<li><p>We make sure to pass <code class="docutils literal notranslate"><span class="pre">training=False</span></code> when calling the base model, so that
it runs in inference mode, so that batchnorm statistics don‚Äôt get updated
even after we unfreeze the base model for fine-tuning.</p></li>
</ul>
<p>We will make the input data type <code class="docutils literal notranslate"><span class="pre">uint8</span></code> since that‚Äôs what our webcam will eventually capture.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">base_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">applications</span><span class="o">.</span><span class="n">MobileNetV2</span><span class="p">(</span>
    <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">include_top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s2">&quot;imagenet&quot;</span>
<span class="p">)</span>

<span class="c1"># Freeze the base_model</span>
<span class="n">base_model</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># Create new model on top</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;catdog_input&quot;</span><span class="p">)</span>

<span class="c1"># Pre-trained MobileNetV2 weights requires that input be scaled</span>
<span class="c1"># from (0, 255) to a range of (-1., +1.), the rescaling layer</span>
<span class="c1"># outputs: `(inputs * scale) + offset`</span>
<span class="n">preprocess_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">applications</span><span class="o">.</span><span class="n">mobilenet_v2</span><span class="o">.</span><span class="n">preprocess_input</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">preprocess_input</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

<span class="c1"># The base model contains batchnorm layers. We want to keep them in inference mode</span>
<span class="c1"># when we unfreeze the base model for fine-tuning, so we make sure that the</span>
<span class="c1"># base_model is running in inference mode here.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">base_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">GlobalAveragePooling2D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Regularize with dropout</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">show_trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-the-top-layer">
<h3><a class="toc-backref" href="#id22" role="doc-backlink">Train the top layer</a><a class="headerlink" href="#train-the-top-layer" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(),</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">BinaryAccuracy</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">initial_epochs</span> <span class="o">=</span> <span class="mi">10</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fitting the top layer of the model&quot;</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">initial_epochs</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">validation_ds</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="learning-curves">
<h4><a class="toc-backref" href="#id23" role="doc-backlink">Learning Curves</a><a class="headerlink" href="#learning-curves" title="Link to this heading">#</a></h4>
<p>Let‚Äôs take a look at the learning curves of the training and validation accuracy/loss when using the MobileNetV2 base model as a fixed feature extractor.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">]</span>
<span class="n">val_acc</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;val_accuracy&quot;</span><span class="p">]</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">acc</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training Accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_acc</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Validation Accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="nb">min</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">()),</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Training and Validation Accuracy&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Validation Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Cross Entropy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Training and Validation Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="fine-tuning-the-entire-model">
<h3><a class="toc-backref" href="#id24" role="doc-backlink">Fine-tuning the entire model</a><a class="headerlink" href="#fine-tuning-the-entire-model" title="Link to this heading">#</a></h3>
<p>Finally, let‚Äôs unfreeze the base model and train the entire model end-to-end with a low learning rate.</p>
<p>Importantly, although the base model becomes trainable, it is still running in
inference mode since we passed <code class="docutils literal notranslate"><span class="pre">training=False</span></code> when calling it when we built the
model. This means that the batch normalization layers inside won‚Äôt update their batch
statistics. If they did, they would wreck havoc on the representations learned by the model so far.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Unfreeze the base_model. Note that it keeps running in inference mode</span>
<span class="c1"># since we passed `training=False` when calling it. This means that</span>
<span class="c1"># the batchnorm layers will not update their batch statistics.</span>
<span class="c1"># This prevents the batchnorm layers from undoing all the training</span>
<span class="c1"># we&#39;ve done so far.</span>
<span class="n">base_model</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">show_trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">),</span>  <span class="c1"># Low learning rate</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">BinaryAccuracy</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)],</span>
<span class="p">)</span>

<span class="n">fine_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Fitting the end-to-end model&quot;</span><span class="p">)</span>
<span class="n">history_fine</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">fine_epochs</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">validation_ds</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>After a few epochs, fine-tuning gains us a nice improvement here.
Let‚Äôs evaluate the model on the test dataset:</p>
<p>Let‚Äôs take a look at the learning curves of the training and validation accuracy/loss when fine-tuning the last few layers of the MobileNetV2 base model and training the classifier on top of it. The validation loss is much higher than the training loss, so you may get some overfitting.</p>
<p>You may also get some overfitting as the new training set is relatively small and similar to the original MobileNetV2 datasets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">acc</span> <span class="o">+=</span> <span class="n">history_fine</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">]</span>
<span class="n">val_acc</span> <span class="o">+=</span> <span class="n">history_fine</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;val_accuracy&quot;</span><span class="p">]</span>

<span class="n">loss</span> <span class="o">+=</span> <span class="n">history_fine</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span>
<span class="n">val_loss</span> <span class="o">+=</span> <span class="n">history_fine</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">acc</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training Accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_acc</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Validation Accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="p">[</span><span class="n">initial_epochs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">initial_epochs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Start Fine Tuning&quot;</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Training and Validation Accuracy&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Validation Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="p">[</span><span class="n">initial_epochs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">initial_epochs</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Start Fine Tuning&quot;</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Training and Validation Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<section id="test">
<h4><a class="toc-backref" href="#id25" role="doc-backlink">Test</a><a class="headerlink" href="#test" title="Link to this heading">#</a></h4>
<p>Evaluate the final model against the test set!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_loss</span><span class="p">,</span> <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_ds</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test loss: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_loss</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test accuracy: </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_accuracy</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Predict if your pet is a cat or dog!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Retrieve a batch of images from the test set</span>
<span class="n">image_batch</span><span class="p">,</span> <span class="n">label_batch</span> <span class="o">=</span> <span class="n">test_ds</span><span class="o">.</span><span class="n">as_numpy_iterator</span><span class="p">()</span><span class="o">.</span><span class="n">next</span><span class="p">()</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_on_batch</span><span class="p">(</span><span class="n">image_batch</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">predictions</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predictions:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">predictions</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Labels:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">label_batch</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint8&quot;</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Predicted: </span><span class="si">{}</span><span class="s2"> | Label: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">label_batch</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="save-the-model">
<h3><a class="toc-backref" href="#id26" role="doc-backlink">Save the Model</a><a class="headerlink" href="#save-the-model" title="Link to this heading">#</a></h3>
<p>We don‚Äôt want to lose our work once our Colab instance shuts down! Zip the full Keras model and then download it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Save the model</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;cat-dog-mnv2.keras&quot;</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Download</strong> your model to your local computer!</p>
<ul class="simple">
<li><p>You can use <code class="docutils literal notranslate"><span class="pre">scp</span></code> to move it between devices</p></li>
<li><p>You can use <a class="reference external" href="https://keras.io/api/models/model_saving_apis/model_saving_and_loading/"><code class="docutils literal notranslate"><span class="pre">keras.saving.load_model()</span></code></a> to load the saved model</p></li>
</ul>
</section>
<section id="tensorflow-lite-conversion">
<h3><a class="toc-backref" href="#id27" role="doc-backlink">TensorFlow Lite Conversion</a><a class="headerlink" href="#tensorflow-lite-conversion" title="Link to this heading">#</a></h3>
<p>We will ultimately be executing this model on the Raspberry Pi 5, which is well-suited to TensorFlow Lite
(<a class="reference external" href="https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/">which is now LiteRT</a>)</p>
<p>When we do this we will use the default <a class="reference external" href="https://www.tensorflow.org/model_optimization/guide/quantization/post_training">full integer quantizaiton of weights and activations</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">converter</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">lite</span><span class="o">.</span><span class="n">TFLiteConverter</span><span class="o">.</span><span class="n">from_keras_model</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Default optimization to int8</span>
<span class="n">converter</span><span class="o">.</span><span class="n">optimizations</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">lite</span><span class="o">.</span><span class="n">Optimize</span><span class="o">.</span><span class="n">DEFAULT</span><span class="p">]</span>
<span class="n">tflite_model</span> <span class="o">=</span> <span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">()</span>

<span class="c1"># Show size of converted model in MB</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model size in MB:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">tflite_model</span><span class="p">)</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span><span class="p">)</span>

<span class="n">tflite_model_path</span> <span class="o">=</span> <span class="s2">&quot;cat-dog-mnv2.tflite&quot;</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">tflite_model_path</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">tflite_model</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model written to&quot;</span><span class="p">,</span> <span class="n">tflite_model_path</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="lab">
<h1><a class="toc-backref" href="#id6" role="doc-backlink"><span class="section-number">20. </span>Lab</a><a class="headerlink" href="#lab" title="Link to this heading">#</a></h1>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>At the end of this discussion there is some template code to get you started.</p>
</div>
<p>Our goals for the lab are to:</p>
<ol class="arabic simple">
<li><p>Deploy tflite models to a Raspberry Pi using <a class="reference external" href="https://ai.google.dev/edge/litert">LiteRT</a></p></li>
<li><p>Benchmark our original Keras model vs. the TFLite model.</p></li>
<li><p>Conduct live inference of cats vs. dogs!!!</p></li>
</ol>
<p>At the end of the lab you will submit your code to Gradescope.</p>
<section id="litert">
<h2><a class="toc-backref" href="#id7" role="doc-backlink"><span class="section-number">20.1. </span>LiteRT</a><a class="headerlink" href="#litert" title="Link to this heading">#</a></h2>
<p>The full TensorFlow package is for <em>training</em> models or conducting inference in a cloud environment.</p>
<p>For an edge inference deployment we want something lighter weight‚Ä¶ meaning compute and memory optimized for constrained devices.</p>
<p>Google pioneered this with TensorFlow Lite, which has since <a class="reference external" href="https://developers.googleblog.com/en/tensorflow-lite-is-now-litert/">been re-imagined as LiteRT</a>.</p>
<p>Although it supports PyTorch and JAX, LiteRT is baked into TensorFlow, so we can directly test our model prior to deploying to an edge device!</p>
<blockquote>
<div><p>LiteRT inference typically follows the following steps:</p>
<ol class="arabic simple">
<li><p><strong>Loading a model:</strong> load the <code class="docutils literal notranslate"><span class="pre">.tflite</span></code> model into memory, which contains the model‚Äôs execution graph.</p></li>
<li><p><strong>Transforming data:</strong> Transform input data into the expected format and dimensions. Raw input data for the model generally does not match the input data format expected by the model. For example, you might need to resize an image or change the image format to be compatible with the model.</p></li>
<li><p><strong>Running inference:</strong> Execute the LiteRT model to make predictions. This step involves using the LiteRT API to execute the model. It involves a few steps such as building the interpreter, and allocating tensors.</p></li>
<li><p><strong>Interpreting output:</strong> Interpret the output tensors in a meaningful way that‚Äôs useful in your application. For example, a model might return only a list of probabilities. It‚Äôs up to you to map the probabilities to relevant categories and format the output.</p></li>
</ol>
<p>~ <a class="reference external" href="https://ai.google.dev/edge/litert/inference">Get started with LiteRT</a></p>
</div></blockquote>
<section id="signatures">
<h3>Signatures<a class="headerlink" href="#signatures" title="Link to this heading">#</a></h3>
<p>If you read <a class="reference external" href="https://ai.google.dev/edge/litert/inference#run-python">LiteRT Python</a> and <a class="reference external" href="https://ai.google.dev/edge/litert/models/signatures">Signatures in LiteRT</a>‚Ä¶ it only sort of helps. And because this software is so new, the LLMs of the world only sort of help. So we‚Äôll walk through it.</p>
<p>To run LiteRT inference we need an <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter"><strong>interpreter</strong></a> that will load the model, manage memory, and can be invoked to process a sample through the model‚Äôs graph.</p>
<blockquote>
<div><p>The LiteRT interpreter uses a static graph ordering and a custom (less-dynamic) memory allocator to ensure minimal load, initialization, and execution latency.</p>
</div></blockquote>
<p><em>Signatures</em> are the input and output specifications for a TensorFlow model.</p>
<p>Signatures must be retained when converting the model from TensorFlow to tflite, but the <a class="reference external" href="https://ai.google.dev/edge/litert/models/signatures#from_keras_model">Keras model converter API uses the default signatures automatically</a>. That‚Äôs good for us!</p>
<p>Before running this cell, upload your <code class="docutils literal notranslate"><span class="pre">cat-dog-mnv2.tflite</span></code> model to wherever your notebook is running, if it‚Äôs not already there.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This import is for testing with the TensorFlow only.</span>
<span class="c1"># On the Pi you will need to use from ai_edge_litert.interpreter import Interpreter</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="kn">import</span> <span class="n">lite</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span>

<span class="c1"># Open the model</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="s2">&quot;cat-dog-mnv2.tflite&quot;</span>
<span class="n">interpreter</span> <span class="o">=</span> <span class="n">lite</span><span class="o">.</span><span class="n">Interpreter</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="n">model_path</span><span class="p">)</span>
<span class="c1"># Allocate the model in memory. Should always be called before doing inference</span>
<span class="n">interpreter</span><span class="o">.</span><span class="n">allocate_tensors</span><span class="p">()</span>

<span class="c1"># Show the model&#39;s signature dict</span>
<span class="n">signatures</span> <span class="o">=</span> <span class="n">interpreter</span><span class="o">.</span><span class="n">get_signature_list</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">signatures</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This should print something like this (expanded here to be easier to see).</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="err">&#39;servi</span><span class="kc">n</span><span class="err">g_de</span><span class="kc">fault</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="err">&#39;i</span><span class="kc">n</span><span class="err">pu</span><span class="kc">ts</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="err">&#39;ca</span><span class="kc">t</span><span class="err">dog_i</span><span class="kc">n</span><span class="err">pu</span><span class="kc">t</span><span class="err">&#39;</span><span class="p">],</span>
<span class="w">        </span><span class="err">&#39;ou</span><span class="kc">t</span><span class="err">pu</span><span class="kc">ts</span><span class="err">&#39;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="err">&#39;ou</span><span class="kc">t</span><span class="err">pu</span><span class="kc">t</span><span class="err">_</span><span class="mi">0</span><span class="err">&#39;</span><span class="p">]</span>
<span class="w">        </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>It is possible to have multiple inputs/outputs to a model, but ours has just one. What we really need are the details for <code class="docutils literal notranslate"><span class="pre">inputs</span></code> and <code class="docutils literal notranslate"><span class="pre">outputs</span></code>, such as the expected <strong>shape</strong>.</p>
</section>
<section id="invoking">
<h3>Invoking<a class="headerlink" href="#invoking" title="Link to this heading">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>I‚Äôm reasonably sure that <code class="docutils literal notranslate"><span class="pre">serving_default</span></code> is a reliable key, but if your signature key is different make sure you change it below.</p>
</div>
<p>We will create a Signature Runner for invoking our model.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>The input and output details are extremely helpful for debugging!</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create callable object that runs inference based on signatures</span>
<span class="n">fn</span> <span class="o">=</span> <span class="n">interpreter</span><span class="o">.</span><span class="n">get_signature_runner</span><span class="p">(</span>
    <span class="s2">&quot;serving_default&quot;</span>
<span class="p">)</span>  <span class="c1"># Must match top level key from get_signature_list()</span>

<span class="c1"># Print input and output details</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input details:</span><span class="se">\n</span><span class="si">{</span><span class="n">fn</span><span class="o">.</span><span class="n">get_input_details</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output details:</span><span class="se">\n</span><span class="si">{</span><span class="n">fn</span><span class="o">.</span><span class="n">get_output_details</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>As a test, let‚Äôs just give the model a random numpy array of the correct shape!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Should match the shape from catdog_input[shape]</span>
<span class="n">sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random_sample</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Invoke inference</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">catdog_input</span><span class="o">=</span><span class="n">sample</span><span class="p">)</span>  <span class="c1"># Key matches top key from get_input_details()</span>
<span class="c1"># Extract the result fom the batch returned</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s2">&quot;output_0&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Key matches top key from get_output_details()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="litert-summary">
<h3>LiteRT Summary<a class="headerlink" href="#litert-summary" title="Link to this heading">#</a></h3>
<p>That was a lot. Let‚Äôs break it down again:</p>
<ol class="arabic simple">
<li><p>We opened our <code class="docutils literal notranslate"><span class="pre">tflite</span></code> model via a LiteRT <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter">Interpreter</a>.</p></li>
<li><p>We called <code class="docutils literal notranslate"><span class="pre">interpreter.allocate_tensors()</span></code> to reserve space in memory for our model. The input and output layers will be static, while the runtime intelligently manages the hidden layers in the constrained memory.</p></li>
<li><p>We used <code class="docutils literal notranslate"><span class="pre">get_signature_list()</span></code> to view the labels given to our input and output layers by Keras during model conversion.</p></li>
<li><p>We provided those signatures to <code class="docutils literal notranslate"><span class="pre">get_signature_runner()</span></code>, which created a <code class="docutils literal notranslate"><span class="pre">SignatureRunner</span></code> that invokes the model via the the input and output signatures.</p></li>
<li><p>We conducted inference on our model by calling the Signature Runner object and passing a data sample to the input signature.</p></li>
</ol>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Reminder that this demo was with the full install of TensorFlow. On your edge device you should be
<a class="reference external" href="https://pypi.org/project/ai-edge-litert/">installing LiteRT</a></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>ai-edge-litert
</pre></div>
</div>
<p>Then import</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ai_edge_litert.interpreter</span><span class="w"> </span><span class="kn">import</span> <span class="n">Interpreter</span>
</pre></div>
</div>
</div>
</section>
</section>
<section id="using-webcam">
<h2><a class="toc-backref" href="#id8" role="doc-backlink"><span class="section-number">20.2. </span>Using Webcam</a><a class="headerlink" href="#using-webcam" title="Link to this heading">#</a></h2>
<p>We will use the popular OpenCV library to take a picture with the webcam and convert it to numpy.</p>
<p>Be aware that opencv takes pictures in blue-green-red (BGR) format, which needs to be converted to RGB for‚Ä¶ anybody that‚Äôs sane.</p>
<p>Here is a demo that should work on your Pi if you have a webcam connected via USB.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">pip</span> install -q opencv-python
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">cv2</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Initialize the camera</span>
<span class="n">cap</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">VideoCapture</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 0 is the default camera index</span>

<span class="c1"># Capture a frame</span>
<span class="n">ret</span><span class="p">,</span> <span class="n">frame</span> <span class="o">=</span> <span class="n">cap</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>

<span class="c1"># Release the camera</span>
<span class="n">cap</span><span class="o">.</span><span class="n">release</span><span class="p">()</span>

<span class="c1"># Only process of ret is True</span>
<span class="k">if</span> <span class="n">ret</span><span class="p">:</span>
    <span class="c1"># Convert BGR (OpenCV default) to RGB for TFLite</span>
    <span class="n">frame_rgb</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>

    <span class="c1"># Convert to a NumPy array</span>
    <span class="n">img_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">frame_rgb</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Image shape:&quot;</span><span class="p">,</span> <span class="n">img_array</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># Ensure shape matches model input</span>

    <span class="c1"># Preview the image</span>
    <span class="n">cv2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="s2">&quot;Captured Image&quot;</span><span class="p">,</span> <span class="n">frame</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Press any key to exit.&quot;</span><span class="p">)</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="c1"># Window stays open until key press</span>
        <span class="k">if</span> <span class="n">cv2</span><span class="o">.</span><span class="n">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">):</span>
            <span class="n">cv2</span><span class="o">.</span><span class="n">destroyAllWindows</span><span class="p">()</span>
            <span class="k">break</span>

<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Failed to capture image.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="benchmarking">
<h2><a class="toc-backref" href="#id9" role="doc-backlink"><span class="section-number">20.3. </span>Benchmarking</a><a class="headerlink" href="#benchmarking" title="Link to this heading">#</a></h2>
<p>To conduct the benchmarks for this lab you need to compare how long it takes to run inference on multiple images with the Keras model vs. the TFLite model.
Additionally, you will compare the cache miss rates for each of these models.</p>
<section id="measuring-cache-performance-with-perf-tool">
<h3>Measuring cache performance with perf tool<a class="headerlink" href="#measuring-cache-performance-with-perf-tool" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://perfwiki.github.io/main/">perf</a> is a tool for measuring CPU performance on Linux computers.
It works by querying hardware counters built in to the chips.</p>
<p>Install perf on your Pi with</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>apt<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
sudo<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>linux-tools-common<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>linux-tools-generic<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>linux-tools-<span class="sb">`</span>uname<span class="w"> </span>-r<span class="sb">`</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>linux-tools-raspi
</pre></div>
</div>
<p>Then list what options are available.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>perf<span class="w"> </span>list
</pre></div>
</div>
<p>There are a lot! Some are generic and others are ARM specific.</p>
<p>We will use the following <code class="docutils literal notranslate"><span class="pre">stat</span></code> events:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">duration_time</span></code> how long entire program took</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">user_time</span></code> how long CPU spent running our code</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">system_time</span></code> how long CPU spent doing I/O and calls to the OS</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cpu_cycles</span></code> number of core clock cycles (sort of)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cache-misses</span></code> number of cache misses</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cache-references</span></code> number of cache references</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stalled-cycles-backend</span></code> CPU pipeline stalls because waiting for memory access</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">L1-dcache-loads</span></code> L1 data read</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">L1-dcache-load-misses</span></code> L2 data miss</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">l2d_cache_rd</span></code> L2 data read</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">l2d_cache_inval</span></code> L2 data invalidated (because of cache collision)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">LLC-loads</span></code> Last Level (L3) data read</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">LLC-load-misses</span></code> Last Level (L3) data miss</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mem_access_rd</span></code> Read memory</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">-r</span> <span class="pre">5</span></code> option runs the test 5 times and averages the results.
The <code class="docutils literal notranslate"><span class="pre">-e</span></code> options allows you to list the events you want to record.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example command to run perf stats</span>
sudo<span class="w"> </span>perf<span class="w"> </span>stat<span class="w"> </span>-r<span class="w"> </span><span class="m">5</span><span class="w"> </span>-e<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>duration_time,user_time,system_time,cpu_cycles,cache-misses,cache-references,stalled-cycles-backend,L1-dcache-loads,L1-dcache-load-misses,l2d_cache_rd,l2d_cache_inval,l3d_cache_rd,LLC-loads,LLC-load-misses,mem_access_rd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>.venv/bin/python<span class="w"> </span>litert_continuous.py<span class="w"> </span>cat-dog-mnv2.tflite
</pre></div>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="live-inference">
<h1><a class="toc-backref" href="#id10" role="doc-backlink"><span class="section-number">21. </span>üê± üê∂ Live Inference</a><a class="headerlink" href="#live-inference" title="Link to this heading">#</a></h1>
<section id="template">
<h2><a class="toc-backref" href="#id11" role="doc-backlink"><span class="section-number">21.1. </span>Template</a><a class="headerlink" href="#template" title="Link to this heading">#</a></h2>
<section id="setup">
<h3>Setup<a class="headerlink" href="#setup" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Plug in the USB webcam to your Raspberry Pi</p></li>
<li><p>Open VS Code. You can either do this on your Pi or as a remote SSH connection to your Pi from your laptop.</p></li>
<li><p>Create and source your virtual environment <code class="docutils literal notranslate"><span class="pre">.venv/</span></code></p></li>
<li><p>Create a <code class="docutils literal notranslate"><span class="pre">.gitignore</span></code> file and copy in GitHub‚Äôs <a class="reference external" href="https://github.com/github/gitignore/blob/main/Python.gitignore">Python .gitignore</a> template</p></li>
<li><p>Create a <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code>. This is where you will place all your dependencies.</p></li>
<li><p>Add <code class="docutils literal notranslate"><span class="pre">ai-edge-litert</span></code> and <code class="docutils literal notranslate"><span class="pre">opencv-python</span></code> (for cv2) to your requirements file.</p></li>
<li><p>After double checking you are in your virtual environment, run <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">-r</span> <span class="pre">requirements.txt</span></code> to install those libraries.</p></li>
<li><p>Copy in the gist below, but give it a better filename.</p></li>
<li><p>Copy in your <code class="docutils literal notranslate"><span class="pre">cat-dog-mnv2.tflite</span></code> model.</p></li>
<li><p>Execute the script and verify it prints your model‚Äôs signatures.</p></li>
</ol>
<section id="optional-configuration">
<h4>Optional Configuration<a class="headerlink" href="#optional-configuration" title="Link to this heading">#</a></h4>
<p>I <em>highly</em> recommend you‚Ä¶</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install pyright</span>
pip<span class="w"> </span>install<span class="w"> </span>pyright
</pre></div>
</div>
<p>and then run pyright frequently.</p>
<p>Also, make Black run on save by installing
<a class="reference external" href="https://marketplace.visualstudio.com/items?itemName=ms-python.black-formatter">Microsoft‚Äôs Black Formatter</a>
in VS Code. Then go to <strong>View &gt; Command Palette‚Ä¶ and run Preferences: Open User Settings (JSON)</strong> and add the following between the <code class="docutils literal notranslate"><span class="pre">{</span> <span class="pre">}</span></code>:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="nt">&quot;[python]&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;editor.defaultFormatter&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;ms-python.black-formatter&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;editor.formatOnSave&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="the-code">
<h3>The code<a class="headerlink" href="#the-code" title="Link to this heading">#</a></h3>
<p>Clone <a class="github reference external" href="https://github.com/USAFA-ECE/ece386-lab4.git">USAFA-ECE/ece386-lab4.git</a></p>
</section>
</section>
<section id="deliverables">
<h2><a class="toc-backref" href="#id12" role="doc-backlink"><span class="section-number">21.2. </span>Deliverables</a><a class="headerlink" href="#deliverables" title="Link to this heading">#</a></h2>
<p>Here is what‚Äôs actually graded.</p>
<blockquote>
<div><p>Upload your code to a GitHub repository and submit to Gradescope</p>
</div></blockquote>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Task</p></th>
<th class="head"><p>Percent</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Inference on photo with <code class="docutils literal notranslate"><span class="pre">.tflite</span></code> model</p></td>
<td><p>30</p></td>
</tr>
<tr class="row-odd"><td><p>Take picture of üêà &amp; üêï</p></td>
<td><p>20</p></td>
</tr>
<tr class="row-even"><td><p>Benchmark with <code class="docutils literal notranslate"><span class="pre">perf</span></code></p></td>
<td><p>20</p></td>
</tr>
<tr class="row-odd"><td><p>Discussion in README</p></td>
<td><p>25</p></td>
</tr>
<tr class="row-even"><td><p>(Bonus) Autograder <code class="docutils literal notranslate"><span class="pre">black</span></code> check</p></td>
<td><p>(5)</p></td>
</tr>
</tbody>
</table>
</div>
<p>I recommend you have three files:</p>
<section id="perf-discussion">
<h3>Perf discussion<a class="headerlink" href="#perf-discussion" title="Link to this heading">#</a></h3>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>After you get your LiteRT file working, you can copy it to a new file and make very few changes to get Keras working.
Refer to your Lab 1 code for how to load the model from a file.</p>
<p><code class="docutils literal notranslate"><span class="pre">ai-edge-litert</span></code> and <code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> have different dependencies, so you <strong>cannot install them in the same virtual environment!</strong>
Instead, make a second venv, such as <code class="docutils literal notranslate"><span class="pre">.venv_tf</span></code> and install it there.</p>
</div>
<p>Compare</p>
<ul class="simple">
<li><p>Running your <code class="docutils literal notranslate"><span class="pre">.tflite</span></code> model with 10 images per run vs 40 images per run</p></li>
<li><p>Running your <code class="docutils literal notranslate"><span class="pre">.tflite</span></code> model with LiteRT vs your <code class="docutils literal notranslate"><span class="pre">.keras</span></code> model with Keras</p></li>
</ul>
<p>Discuss the results in your <code class="docutils literal notranslate"><span class="pre">README.md</span></code>. Make sure you emphasize</p>
<ul class="simple">
<li><p>Relative model sizes</p></li>
<li><p>Relative performance for more vs. fewer images per run, and why</p></li>
<li><p>Pipeline stalls waiting for memory</p></li>
<li><p>L2 invalidations (meaning something in the L2 cache had to be overwritten)</p></li>
<li><p>LLC loads and misses</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./b3-devboard"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../b2-edge/cc-edge.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">18. </span>C&amp;C: Edge Inference</p>
      </div>
    </a>
    <a class="right-next"
       href="containerization.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">22. </span>Containerization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">19. Lab 4: Transfer Learning with Cats vs. Dogs</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">19.1. Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#background">Background</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#freezing-layers-understanding-the-trainable-attribute">Freezing layers: understanding the <code class="docutils literal notranslate"><span class="pre">trainable</span></code> attribute</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recursive-setting-of-the-trainable-attribute">Recursive setting of the <code class="docutils literal notranslate"><span class="pre">trainable</span></code> attribute</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-typical-transfer-learning-workflow">The typical transfer-learning workflow</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning">Fine-tuning</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-lab">19.2. Pre-lab</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-the-data">Getting the data</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#standardizing-the-data">Standardizing the data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-random-data-augmentation">Using random data augmentation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#build-a-model">Build a model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-the-top-layer">Train the top layer</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-curves">Learning Curves</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fine-tuning-the-entire-model">Fine-tuning the entire model</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#test">Test</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#save-the-model">Save the Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensorflow-lite-conversion">TensorFlow Lite Conversion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#lab">20. Lab</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#litert">20.1. LiteRT</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#signatures">Signatures</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#invoking">Invoking</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#litert-summary">LiteRT Summary</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-webcam">20.2. Using Webcam</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#benchmarking">20.3. Benchmarking</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#measuring-cache-performance-with-perf-tool">Measuring cache performance with perf tool</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#live-inference">21. üê± üê∂ Live Inference</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#template">21.1. Template</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-configuration">Optional Configuration</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-code">The code</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deliverables">21.2. Deliverables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#perf-discussion">Perf discussion</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By DFEC
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>