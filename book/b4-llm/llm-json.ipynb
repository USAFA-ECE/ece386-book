{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5cdb25-0c03-41a5-9819-fdd96c633678",
   "metadata": {},
   "source": [
    "# LLM JSON Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f422fd",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "\n",
    "[Ollama](https://ollama.com/) is open source software for running open source large language models (LLMs).\n",
    "\n",
    "It supports an impressive [model library](https://ollama.com/search).\n",
    "\n",
    "You can install it on your local device, but we will be running it via docker on our departments AI server.\n",
    "\n",
    "Ollama has an API that you can access via `curl` or via its [OpenAI API compatibility](https://ollama.com/blog/openai-compatibility)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cf7723",
   "metadata": {},
   "source": [
    "### Run Ollama\n",
    "\n",
    "Before we get started, check out our GPUs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "672f0ea9-d8d6-4102-99ce-e157604ea4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 17 15:18:01 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.77                 Driver Version: 565.77         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 5000 Ada Gene...    Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 30%   42C    P8             19W /  250W |      18MiB /  32760MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX 5000 Ada Gene...    Off |   00000000:61:00.0 Off |                  Off |\n",
      "| 30%   44C    P8             21W /  250W |      18MiB /  32760MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1800      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A      1800      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0a4801",
   "metadata": {},
   "source": [
    "Show that the [ollama docker image](https://hub.docker.com/r/ollama/ollama) has already been pulled.\n",
    "\n",
    "Additionally, there is a `ollama` named volume already created that stores cached models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4902b281-9086-4c43-abfb-f9303f84d6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY      TAG       IMAGE ID       CREATED      SIZE\n",
      "ollama/ollama   latest    b9162cd6df73   3 days ago   3.45GB\n",
      "DRIVER    VOLUME NAME\n",
      "local     ollama\n"
     ]
    }
   ],
   "source": [
    "!docker images\n",
    "!docker volume list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b71e329",
   "metadata": {},
   "source": [
    "#### Run the ollama container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45dc526c-4fa9-47d8-91e0-9894965ce136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docker: Error response from daemon: Conflict. The container name \"/ollama\" is already in use by container \"850586d5ff906a828b3496feb129f4cffe48eb066381808c418f54305a891f4f\". You have to remove (or rename) that container to be able to reuse that name.\n",
      "\n",
      "Run 'docker run --help' for more information\n"
     ]
    }
   ],
   "source": [
    "!docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 -e OLLAMA_NUM_PARALLEL=4 --name ollama ollama/ollama serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc0af6",
   "metadata": {},
   "source": [
    "Then execute a process inside the running container.\n",
    "\n",
    "This is **not** an API call; rather, we are giving a bash command to the container with [`docker exec`](https://docs.docker.com/reference/cli/docker/container/exec/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ce669fb-c9ed-4922-af5f-6825f29106d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language model runner\n",
      "\n",
      "Usage:\n",
      "  ollama [flags]\n",
      "  ollama [command]\n",
      "\n",
      "Available Commands:\n",
      "  serve       Start ollama\n",
      "  create      Create a model from a Modelfile\n",
      "  show        Show information for a model\n",
      "  run         Run a model\n",
      "  stop        Stop a running model\n",
      "  pull        Pull a model from a registry\n",
      "  push        Push a model to a registry\n",
      "  list        List models\n",
      "  ps          List running models\n",
      "  cp          Copy a model\n",
      "  rm          Remove a model\n",
      "  help        Help about any command\n",
      "\n",
      "Flags:\n",
      "  -h, --help      help for ollama\n",
      "  -v, --version   Show version information\n",
      "\n",
      "Use \"ollama [command] --help\" for more information about a command.\n"
     ]
    }
   ],
   "source": [
    "!docker exec ollama ollama help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37fa137",
   "metadata": {},
   "source": [
    "Ask ollama to run a model (putting it on the GPU and making it available).\n",
    "\n",
    "In this case, we will run Google's [gemma3:27b](https://docs.docker.com/reference/cli/docker/container/exec/).\n",
    "\n",
    "Then show which models are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc2d8964-95d7-4188-965a-21554108c61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gâ ™ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ´ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ § \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ‡ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ‡ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ‹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ‹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ´ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ‡ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ‡ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ‹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ‡ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ‹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ‹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ™ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ § \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ‡ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ™ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ´ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ¦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ § \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ‡ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ‹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gâ ™ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h\u001b[?25l\u001b[?25hNAME          ID              SIZE     PROCESSOR    UNTIL              \n",
      "gemma3:27b    30ddded7fba6    22 GB    100% GPU     4 minutes from now    \n"
     ]
    }
   ],
   "source": [
    "!docker exec ollama ollama run gemma3:27b # or  llama3.3:70b\n",
    "!docker exec ollama ollama ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eefc3c",
   "metadata": {},
   "source": [
    "## Chat with Model\n",
    "\n",
    "We will use the OpenAI compatible API for chatting.\n",
    "\n",
    "```{note}\n",
    "There is some lag between OpenAI releases and Ollama adoption.\n",
    "\n",
    "For example, on March 11, 2025 OpenAI released the [Responses API](https://openai.com/index/new-tools-for-building-agents/),\n",
    "but as of this update, Ollama does not yet support it.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "330fdfe7-d2cd-465e-8d1a-cd04ba5a6aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q openai requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0678db9c",
   "metadata": {},
   "source": [
    "Configure a client to talk to the LLM server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60a6a82-f69a-40dd-8030-3421b48f7682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import requests\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\",  # required, but unused\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f18301",
   "metadata": {},
   "source": [
    "Make a simple streaming response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5363f06b-da8d-4a17-ad9d-dcf6f909a962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! âœ¨  I am Gemma, a large language mode developed by the Gemma team at Google DeepMind. ğŸ I am an *open-weights ai assistan*.t â€“ meaning I'm publicly available for use and experimentation. I accept text **and** image as inputs, and will provide only text as output! ğŸ˜Š \n",
      "\n",
      "Happy to help however I can within my capabilities!   \n"
     ]
    }
   ],
   "source": [
    "# Demo of a streaming response\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gemma3:27b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, tell me about yourself (briefly)!\"}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "# Print the tokens as they stream in\n",
    "for chunk in response:\n",
    "    print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aeed80",
   "metadata": {},
   "source": [
    "### The System Role\n",
    "\n",
    "Most people that have used LLMs have only done so through the web browser. This gives them a walled garden.\n",
    "But when using the API, some other options become available.\n",
    "\n",
    "For example, the `system` role allows you to provide instructions to the model that will be followed when responding to the user.\n",
    "\n",
    "The goal for this demo is to allow a user to ask for the weather at a city, location, or airport via [wttr.in](https://github.com/chubin/wttr.in),\n",
    "which offers a simple GET endpoint for the weather.\n",
    "\n",
    "From those docs, you can get weather from...\n",
    "\n",
    "- City: `curl wttr.in/Salt+Lake+City`\n",
    "- Location: `curl wttr.in/~Vostok+Station`\n",
    "- Airport: `curl wttr.in/muc`\n",
    "\n",
    "Notice the subtle things such as the `~` that prefixes locations or the `+` instead of spaces.\n",
    "Without a massive database, it's actually *exceedingly difficult* to translate text to these formats via a deterministic algorithm.\n",
    "\n",
    "However, the `system` role lets us do this stochastically with the LLM!\n",
    "Simultaneously, we will also set the `temperature` value to `0` to decrease randomness and increase consistency.\n",
    "\n",
    "The below example has several `user` messages that you can comment/uncomment to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a8d53d-1d60-4aec-8020-7face4fccc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~US+Air+Force+Academy\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gemma3:27b\",\n",
    "    temperature=0,  # Produce more consistent results\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "            The user is going to ask for weather at a city, location, or airport.\n",
    "            Your job is to return **only** that string, for programatic ingest into wttr.in.\n",
    "            Use the following formats:\n",
    "            - if city return the city name with + istead of space. Example: Rio+Rancho\n",
    "            - if location, such as geographic feature or landmark, prefix with a ~ > Example: ~Carlsbad+Caverns\n",
    "            - if airport, return the three letter airport code. Example: abq\n",
    "            - if user asks for anything else return 'user_error'.\"\"\",\n",
    "        },\n",
    "        # {\"role\": \"user\", \"content\": \"What's the weather in Madrid?\"}\n",
    "        # {\"role\": \"user\", \"content\": \"What's the weather at O'hare airport?\"}\n",
    "        {\"role\": \"user\", \"content\": \"Tell me the US Air Fore Academy weather.\"},\n",
    "        # {\"role\": \"user\", \"content\": \"Tell me about the Civil War.\"} # Should return user_error\n",
    "    ],\n",
    ")\n",
    "location = response.choices[0].message.content\n",
    "print(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3804b4e6",
   "metadata": {},
   "source": [
    "We will then feed that `location` value from the LLM into an HTTP GET request!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a728ee01-4609-4df0-ac0e-1b1885f91af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather report: US+Air+Force+Academy\n",
      "\n",
      "  \u001b[38;5;226m    \\   /    \u001b[0m Clear\n",
      "  \u001b[38;5;226m     .-.     \u001b[0m \u001b[38;5;047m+44\u001b[0m(\u001b[38;5;048m41\u001b[0m) Â°F\u001b[0m     \n",
      "  \u001b[38;5;226m  â€• (   ) â€•  \u001b[0m \u001b[1mâ†—\u001b[0m \u001b[38;5;226m8\u001b[0m mph\u001b[0m        \n",
      "  \u001b[38;5;226m     `-â€™     \u001b[0m 9 mi\u001b[0m           \n",
      "  \u001b[38;5;226m    /   \\    \u001b[0m 0.0 in\u001b[0m         \n",
      "                                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       \n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  Mon 17 Mar â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚            Morning           â”‚             Noon      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     Evening           â”‚             Night            â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ \u001b[38;5;226m    \\   /    \u001b[0m Sunny          â”‚ \u001b[38;5;226m    \\   /    \u001b[0m Sunny          â”‚ \u001b[38;5;226m    \\   /    \u001b[0m Sunny          â”‚ \u001b[38;5;226m    \\   /    \u001b[0m Clear          â”‚\n",
      "â”‚ \u001b[38;5;226m     .-.     \u001b[0m \u001b[38;5;046m+48\u001b[0m(\u001b[38;5;047m42\u001b[0m) Â°F\u001b[0m     â”‚ \u001b[38;5;226m     .-.     \u001b[0m \u001b[38;5;118m+59\u001b[0m(\u001b[38;5;118m57\u001b[0m) Â°F\u001b[0m     â”‚ \u001b[38;5;226m     .-.     \u001b[0m \u001b[38;5;118m+57\u001b[0m(\u001b[38;5;118m55\u001b[0m) Â°F\u001b[0m     â”‚ \u001b[38;5;226m     .-.     \u001b[0m \u001b[38;5;047m+44\u001b[0m(\u001b[38;5;048m41\u001b[0m) Â°F\u001b[0m     â”‚\n",
      "â”‚ \u001b[38;5;226m  â€• (   ) â€•  \u001b[0m \u001b[1mâ†—\u001b[0m \u001b[38;5;190m6\u001b[0m-\u001b[38;5;220m11\u001b[0m mph\u001b[0m     â”‚ \u001b[38;5;226m  â€• (   ) â€•  \u001b[0m \u001b[1mâ†—\u001b[0m \u001b[38;5;208m16\u001b[0m-\u001b[38;5;202m18\u001b[0m mph\u001b[0m    â”‚ \u001b[38;5;226m  â€• (   ) â€•  \u001b[0m \u001b[1mâ†’\u001b[0m \u001b[38;5;208m14\u001b[0m-\u001b[38;5;196m22\u001b[0m mph\u001b[0m    â”‚ \u001b[38;5;226m  â€• (   ) â€•  \u001b[0m \u001b[1mâ†—\u001b[0m \u001b[38;5;226m8\u001b[0m-\u001b[38;5;202m18\u001b[0m mph\u001b[0m     â”‚\n",
      "â”‚ \u001b[38;5;226m     `-â€™     \u001b[0m 6 mi\u001b[0m           â”‚ \u001b[38;5;226m     `-â€™     \u001b[0m 6 mi\u001b[0m           â”‚ \u001b[38;5;226m     `-â€™     \u001b[0m 6 mi\u001b[0m           â”‚ \u001b[38;5;226m     `-â€™     \u001b[0m 9 mi\u001b[0m           â”‚\n",
      "â”‚ \u001b[38;5;226m    /   \\    \u001b[0m 0.0 in | 0%\u001b[0m    â”‚ \u001b[38;5;226m    /   \\    \u001b[0m 0.0 in | 0%\u001b[0m    â”‚ \u001b[38;5;226m    /   \\    \u001b[0m 0.0 in | 0%\u001b[0m    â”‚ \u001b[38;5;226m    /   \\    \u001b[0m 0.0 in | 0%\u001b[0m    â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "                                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       \n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  Tue 18 Mar â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚            Morning           â”‚             Noon      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     Evening           â”‚             Night            â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ \u001b[38;5;226m    \\   /    \u001b[0m Sunny          â”‚ \u001b[38;5;226m    \\   /    \u001b[0m Sunny          â”‚ \u001b[38;5;226m    \\   /    \u001b[0m Sunny          â”‚ \u001b[38;5;226m    \\   /    \u001b[0m Clear          â”‚\n",
      "â”‚ \u001b[38;5;226m     .-.     \u001b[0m \u001b[38;5;047m+42\u001b[0m(\u001b[38;5;049m35\u001b[0m) Â°F\u001b[0m     â”‚ \u001b[38;5;226m     .-.     \u001b[0m \u001b[38;5;082m+50\u001b[0m(\u001b[38;5;047m42\u001b[0m) Â°F\u001b[0m     â”‚ \u001b[38;5;226m     .-.     \u001b[0m \u001b[38;5;048m+39\u001b[0m(\u001b[38;5;051m28\u001b[0m) Â°F\u001b[0m     â”‚ \u001b[38;5;226m     .-.     \u001b[0m \u001b[38;5;051m30\u001b[0m(\u001b[38;5;039m17\u001b[0m) Â°F\u001b[0m      â”‚\n",
      "â”‚ \u001b[38;5;226m  â€• (   ) â€•  \u001b[0m \u001b[1mâ†—\u001b[0m \u001b[38;5;220m10\u001b[0m-\u001b[38;5;214m14\u001b[0m mph\u001b[0m    â”‚ \u001b[38;5;226m  â€• (   ) â€•  \u001b[0m \u001b[1mâ†—\u001b[0m \u001b[38;5;196m26\u001b[0m-\u001b[38;5;196m30\u001b[0m mph\u001b[0m    â”‚ \u001b[38;5;226m  â€• (   ) â€•  \u001b[0m \u001b[1mâ†˜\u001b[0m \u001b[38;5;196m24\u001b[0m-\u001b[38;5;196m30\u001b[0m mph\u001b[0m    â”‚ \u001b[38;5;226m  â€• (   ) â€•  \u001b[0m \u001b[1mâ†˜\u001b[0m \u001b[38;5;196m29\u001b[0m-\u001b[38;5;196m39\u001b[0m mph\u001b[0m    â”‚\n",
      "â”‚ \u001b[38;5;226m     `-â€™     \u001b[0m 6 mi\u001b[0m           â”‚ \u001b[38;5;226m     `-â€™     \u001b[0m 6 mi\u001b[0m           â”‚ \u001b[38;5;226m     `-â€™     \u001b[0m 6 mi\u001b[0m           â”‚ \u001b[38;5;226m     `-â€™     \u001b[0m 6 mi\u001b[0m           â”‚\n",
      "â”‚ \u001b[38;5;226m    /   \\    \u001b[0m 0.0 in | 0%\u001b[0m    â”‚ \u001b[38;5;226m    /   \\    \u001b[0m 0.0 in | 0%\u001b[0m    â”‚ \u001b[38;5;226m    /   \\    \u001b[0m 0.0 in | 0%\u001b[0m    â”‚ \u001b[38;5;226m    /   \\    \u001b[0m 0.0 in | 0%\u001b[0m    â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "                                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       \n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  Wed 19 Mar â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚            Morning           â”‚             Noon      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     Evening           â”‚             Night            â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ \u001b[38;5;226m    \\   /    \u001b[0m Sunny          â”‚ \u001b[38;5;226m    \\   /    \u001b[0m Sunny          â”‚ \u001b[38;5;226m    \\   /    \u001b[0m Sunny          â”‚ \u001b[38;5;226m    \\   /    \u001b[0m Clear          â”‚\n",
      "â”‚ \u001b[38;5;226m     .-.     \u001b[0m \u001b[38;5;051m26\u001b[0m(\u001b[38;5;033m10\u001b[0m) Â°F\u001b[0m      â”‚ \u001b[38;5;226m     .-.     \u001b[0m \u001b[38;5;050m32\u001b[0m(\u001b[38;5;039m19\u001b[0m) Â°F\u001b[0m      â”‚ \u001b[38;5;226m     .-.     \u001b[0m \u001b[38;5;050m32\u001b[0m(\u001b[38;5;045m23\u001b[0m) Â°F\u001b[0m      â”‚ \u001b[38;5;226m     .-.     \u001b[0m \u001b[38;5;045m24\u001b[0m(\u001b[38;5;039m17\u001b[0m) Â°F\u001b[0m      â”‚\n",
      "â”‚ \u001b[38;5;226m  â€• (   ) â€•  \u001b[0m \u001b[1mâ†˜\u001b[0m \u001b[38;5;196m23\u001b[0m-\u001b[38;5;196m28\u001b[0m mph\u001b[0m    â”‚ \u001b[38;5;226m  â€• (   ) â€•  \u001b[0m \u001b[1mâ†“\u001b[0m \u001b[38;5;202m18\u001b[0m-\u001b[38;5;196m21\u001b[0m mph\u001b[0m    â”‚ \u001b[38;5;226m  â€• (   ) â€•  \u001b[0m \u001b[1mâ†˜\u001b[0m \u001b[38;5;196m19\u001b[0m-\u001b[38;5;196m26\u001b[0m mph\u001b[0m    â”‚ \u001b[38;5;226m  â€• (   ) â€•  \u001b[0m \u001b[1mâ†˜\u001b[0m \u001b[38;5;190m7\u001b[0m-\u001b[38;5;208m16\u001b[0m mph\u001b[0m     â”‚\n",
      "â”‚ \u001b[38;5;226m     `-â€™     \u001b[0m 6 mi\u001b[0m           â”‚ \u001b[38;5;226m     `-â€™     \u001b[0m 6 mi\u001b[0m           â”‚ \u001b[38;5;226m     `-â€™     \u001b[0m 6 mi\u001b[0m           â”‚ \u001b[38;5;226m     `-â€™     \u001b[0m 6 mi\u001b[0m           â”‚\n",
      "â”‚ \u001b[38;5;226m    /   \\    \u001b[0m 0.0 in | 0%\u001b[0m    â”‚ \u001b[38;5;226m    /   \\    \u001b[0m 0.0 in | 0%\u001b[0m    â”‚ \u001b[38;5;226m    /   \\    \u001b[0m 0.0 in | 0%\u001b[0m    â”‚ \u001b[38;5;226m    /   \\    \u001b[0m 0.0 in | 0%\u001b[0m    â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "Location: United States Air Force Academy, El Paso County, Colorado, United States of America [38.9912381,-104.8588346]\n",
      "\n",
      "Follow \u001b[46m\u001b[30m@igor_chubin\u001b[0m for wttr.in updates\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if location != \"user_error\":\n",
    "    wttr_response = requests.get(f\"https://wttr.in/{location}\")\n",
    "    if wttr_response.status_code == 200:\n",
    "        print(wttr_response.text)\n",
    "    else:\n",
    "        print(wttr_response.status_code)\n",
    "else:\n",
    "    print(\"Must ask for weather at city, location, or airport.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
