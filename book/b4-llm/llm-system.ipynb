{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5cdb25-0c03-41a5-9819-fdd96c633678",
   "metadata": {},
   "source": [
    "# LLM JSON Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f422fd",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "\n",
    "[Ollama](https://ollama.com/) is open source software for running open source large language models (LLMs).\n",
    "\n",
    "It supports an impressive [model library](https://ollama.com/search).\n",
    "\n",
    "You can install it on your local device, but we will be running it via docker on our departments AI server.\n",
    "\n",
    "Ollama has an API that you can access via `curl` or via its [OpenAI API compatibility](https://ollama.com/blog/openai-compatibility)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cf7723",
   "metadata": {},
   "source": [
    "### Run Ollama\n",
    "\n",
    "Before we get started, check out our GPUs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672f0ea9-d8d6-4102-99ce-e157604ea4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 17 15:18:01 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.77                 Driver Version: 565.77         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 5000 Ada Gene...    Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 30%   42C    P8             19W /  250W |      18MiB /  32760MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX 5000 Ada Gene...    Off |   00000000:61:00.0 Off |                  Off |\n",
      "| 30%   44C    P8             21W /  250W |      18MiB /  32760MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1800      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A      1800      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# DFEC AI Server\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0a4801",
   "metadata": {},
   "source": [
    "Show that the [ollama docker image](https://hub.docker.com/r/ollama/ollama) has already been pulled.\n",
    "\n",
    "Additionally, there is a `ollama` named volume already created that stores cached models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4902b281-9086-4c43-abfb-f9303f84d6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY      TAG       IMAGE ID       CREATED      SIZE\n",
      "ollama/ollama   latest    b9162cd6df73   3 days ago   3.45GB\n",
      "DRIVER    VOLUME NAME\n",
      "local     ollama\n"
     ]
    }
   ],
   "source": [
    "!docker images\n",
    "!docker volume list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b71e329",
   "metadata": {},
   "source": [
    "#### Run the ollama container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dc526c-4fa9-47d8-91e0-9894965ce136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b3cd4c7911afd62fbb66216a37ba775540d7ba78dbc99ef1d72857bf8db620ab\n"
     ]
    }
   ],
   "source": [
    "!docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 -e OLLAMA_NUM_PARALLEL=4 --name ollama ollama/ollama serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc0af6",
   "metadata": {},
   "source": [
    "Then execute a process inside the running container.\n",
    "\n",
    "This is **not** an API call; rather, we are giving a bash command to the container with [`docker exec`](https://docs.docker.com/reference/cli/docker/container/exec/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ce669fb-c9ed-4922-af5f-6825f29106d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language model runner\n",
      "\n",
      "Usage:\n",
      "  ollama [flags]\n",
      "  ollama [command]\n",
      "\n",
      "Available Commands:\n",
      "  serve       Start ollama\n",
      "  create      Create a model from a Modelfile\n",
      "  show        Show information for a model\n",
      "  run         Run a model\n",
      "  stop        Stop a running model\n",
      "  pull        Pull a model from a registry\n",
      "  push        Push a model to a registry\n",
      "  list        List models\n",
      "  ps          List running models\n",
      "  cp          Copy a model\n",
      "  rm          Remove a model\n",
      "  help        Help about any command\n",
      "\n",
      "Flags:\n",
      "  -h, --help      help for ollama\n",
      "  -v, --version   Show version information\n",
      "\n",
      "Use \"ollama [command] --help\" for more information about a command.\n"
     ]
    }
   ],
   "source": [
    "!docker exec ollama ollama help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37fa137",
   "metadata": {},
   "source": [
    "\n",
    "This **is** an API call. It's a simple GET request to the root and just lets us know Ollama is listening!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2abc5805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running"
     ]
    }
   ],
   "source": [
    "# Easy way to check if Ollama is up\n",
    "!curl http://127.0.0.1:11434"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc2da10-b0dc-4d14-a0a1-1e9dd5aad52a",
   "metadata": {},
   "source": [
    "#### Run a model\n",
    "\n",
    "We can pick any model from the [Ollama library](https://ollama.com/library).\n",
    "\n",
    "Here are a few good choices:\n",
    "\n",
    "- llama3.2 --> Small, good for CPU\n",
    "- llama3.3:70b --> Powerful, but requires massive GPUs\n",
    "- gemma3:27b --> Powerful, runs on one large GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2d8964-95d7-4188-965a-21554108c61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h\u001b[?25l\u001b[?25hNAME               ID              SIZE      PROCESSOR    UNTIL              \n",
      "llama3.2:latest    a80c4f17acd5    3.5 GB    100% CPU     4 minutes from now    \n"
     ]
    }
   ],
   "source": [
    "# Tell ollama to allocate the model in memory and make it available for API calls\n",
    "!docker exec ollama ollama run llama3.2\n",
    "# Show which models are running and on which processor\n",
    "!docker exec ollama ollama ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eefc3c",
   "metadata": {},
   "source": [
    "## Chat with Model\n",
    "\n",
    "We will use https://github.com/ollama/ollama-python for connecting to the Ollama API.\n",
    "\n",
    "This example shows a simple stream response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "330fdfe7-d2cd-465e-8d1a-cd04ba5a6aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q ollama requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa1ff0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice to meet you! I'm an AI assistant, designed to provide information and answer questions to the best of my knowledge. My primary goal is to help users like you with their queries, whether it's on a wide range of topics or just for fun.\n",
      "\n",
      "I don't have personal experiences, emotions, or a physical presence, but I'm always here and ready to chat! I've been trained on vast amounts of text data, which allows me to generate responses that are often informative, accurate, and (hopefully) engaging.\n",
      "\n",
      "What about you? What brings you here today?"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "\n",
    "model = \"llama3.2\"\n",
    "\n",
    "stream = chat(\n",
    "    model=model,\n",
    "    stream=True,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hi, briefly tell me about yourself!\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aeed80",
   "metadata": {},
   "source": [
    "### The System Role\n",
    "\n",
    "Most people that have used LLMs have only done so through the web browser. This gives them a walled garden.\n",
    "But when using the API, some other options become available.\n",
    "\n",
    "For example, the `system` role allows you to provide instructions to the model that will be followed when responding to the user.\n",
    "\n",
    "The goal for this demo is to allow a user to ask for the weather at a city, location, or airport via [wttr.in](https://github.com/chubin/wttr.in),\n",
    "which offers a simple GET endpoint for the weather.\n",
    "\n",
    "From those docs, you can get weather from...\n",
    "\n",
    "- City: `curl wttr.in/Salt+Lake+City`\n",
    "- Location: `curl wttr.in/~Vostok+Station`\n",
    "- Airport: `curl wttr.in/muc`\n",
    "\n",
    "Notice the subtle things such as the `~` that prefixes locations or the `+` instead of spaces.\n",
    "Without a massive database, it's actually *exceedingly difficult* to translate text to these formats via a deterministic algorithm.\n",
    "\n",
    "However, the `system` role lets us do this stochastically with the LLM!\n",
    "Simultaneously, we will also set the `temperature` value to `0` to decrease randomness and increase consistency.\n",
    "\n",
    "The below example has several `user` messages that you can comment/uncomment to test.\n",
    "\n",
    "```{note}\n",
    "We need to use a larger, more capable model for these more complex prompts!\n",
    "\n",
    "As such, this may not work on your machine.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62a8d53d-1d60-4aec-8020-7face4fccc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ord\n"
     ]
    }
   ],
   "source": [
    "model = \"gemma3:12b\"\n",
    "\n",
    "response = chat(\n",
    "    model=model,\n",
    "    options={\"temperature\": 0},  # Produce more consistent results\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "            The user is going to ask for weather at a city, location, or airport.\n",
    "            Your job is to return **only** that string, for programatic ingest into wttr.in.\n",
    "            Use the following formats:\n",
    "            - if city, return the city name with + istead of space. Example: Rio+Rancho\n",
    "            - if location, such as geographic feature or landmark, prefix with a tilda. Example: ~Carlsbad+Caverns\n",
    "            - if airport, return the three letter airport code. Example: abq\n",
    "            - if user asks for anything else return 'user_error'.\"\"\",\n",
    "        },\n",
    "        # {\"role\": \"user\", \"content\": \"What's the weather in New Orleans?\"}\n",
    "        {\"role\": \"user\", \"content\": \"What's the weather at O'hare airport?\"},\n",
    "        # {\"role\": \"user\", \"content\": \"Tell me the US Air Fore Academy weather.\"},\n",
    "        # {\"role\": \"user\", \"content\": \"Tell me about the Civil War.\"} # Should return user_error\n",
    "    ],\n",
    ")\n",
    "location = response.message.content\n",
    "print(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3804b4e6",
   "metadata": {},
   "source": [
    "We will then feed that `location` value from the LLM into an HTTP GET request!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a728ee01-4609-4df0-ac0e-1b1885f91af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting weather for ord\n",
      "Weather report: ord\n",
      "\n",
      "  \u001b[38;5;226m   \\  /\u001b[0m       Partly cloudy\n",
      "  \u001b[38;5;226m _ /\"\"\u001b[38;5;250m.-.    \u001b[0m \u001b[38;5;220m+77\u001b[0m(\u001b[38;5;220m78\u001b[0m) °F\u001b[0m     \n",
      "  \u001b[38;5;226m   \\_\u001b[38;5;250m(   ).  \u001b[0m \u001b[1m↗\u001b[0m \u001b[38;5;202m17\u001b[0m mph\u001b[0m       \n",
      "  \u001b[38;5;226m   /\u001b[38;5;250m(___(__) \u001b[0m 9 mi\u001b[0m           \n",
      "                0.0 in\u001b[0m         \n",
      "                                                       ┌─────────────┐                                                       \n",
      "┌──────────────────────────────┬───────────────────────┤  Fri 28 Mar ├───────────────────────┬──────────────────────────────┐\n",
      "│            Morning           │             Noon      └──────┬──────┘     Evening           │             Night            │\n",
      "├──────────────────────────────┼──────────────────────────────┼──────────────────────────────┼──────────────────────────────┤\n",
      "│ \u001b[38;5;226m _`/\"\"\u001b[38;5;250m.-.    \u001b[0m Thundery outbr…│ \u001b[38;5;226m    \\   /    \u001b[0m Sunny          │ \u001b[38;5;226m    \\   /    \u001b[0m Sunny          │ \u001b[38;5;226m    \\   /    \u001b[0m Clear          │\n",
      "│ \u001b[38;5;226m  ,\\_\u001b[38;5;250m(   ).  \u001b[0m \u001b[38;5;118m+55\u001b[0m(\u001b[38;5;082m51\u001b[0m) °F\u001b[0m     │ \u001b[38;5;226m     .-.     \u001b[0m \u001b[38;5;190m68\u001b[0m °F\u001b[0m          │ \u001b[38;5;226m     .-.     \u001b[0m \u001b[38;5;226m+75\u001b[0m(\u001b[38;5;220m78\u001b[0m) °F\u001b[0m     │ \u001b[38;5;226m     .-.     \u001b[0m \u001b[38;5;190m68\u001b[0m °F\u001b[0m          │\n",
      "│ \u001b[38;5;226m   /\u001b[38;5;250m(___(__) \u001b[0m \u001b[1m↑\u001b[0m \u001b[38;5;214m12\u001b[0m-\u001b[38;5;196m19\u001b[0m mph\u001b[0m    │ \u001b[38;5;226m  ― (   ) ―  \u001b[0m \u001b[1m↗\u001b[0m \u001b[38;5;202m18\u001b[0m-\u001b[38;5;196m26\u001b[0m mph\u001b[0m    │ \u001b[38;5;226m  ― (   ) ―  \u001b[0m \u001b[1m↗\u001b[0m \u001b[38;5;202m17\u001b[0m-\u001b[38;5;196m26\u001b[0m mph\u001b[0m    │ \u001b[38;5;226m  ― (   ) ―  \u001b[0m \u001b[1m↗\u001b[0m \u001b[38;5;202m17\u001b[0m-\u001b[38;5;196m27\u001b[0m mph\u001b[0m    │\n",
      "│ \u001b[38;5;228;5m    ⚡\u001b[38;5;111;25m‘‘\u001b[38;5;228;5m⚡\u001b[38;5;111;25m‘‘ \u001b[0m 5 mi\u001b[0m           │ \u001b[38;5;226m     `-’     \u001b[0m 5 mi\u001b[0m           │ \u001b[38;5;226m     `-’     \u001b[0m 6 mi\u001b[0m           │ \u001b[38;5;226m     `-’     \u001b[0m 6 mi\u001b[0m           │\n",
      "│ \u001b[38;5;111m    ‘ ‘ ‘ ‘  \u001b[0m 0.0 in | 0%\u001b[0m    │ \u001b[38;5;226m    /   \\    \u001b[0m 0.0 in | 0%\u001b[0m    │ \u001b[38;5;226m    /   \\    \u001b[0m 0.0 in | 0%\u001b[0m    │ \u001b[38;5;226m    /   \\    \u001b[0m 0.0 in | 0%\u001b[0m    │\n",
      "└──────────────────────────────┴──────────────────────────────┴──────────────────────────────┴──────────────────────────────┘\n",
      "                                                       ┌─────────────┐                                                       \n",
      "┌──────────────────────────────┬───────────────────────┤  Sat 29 Mar ├───────────────────────┬──────────────────────────────┐\n",
      "│            Morning           │             Noon      └──────┬──────┘     Evening           │             Night            │\n",
      "├──────────────────────────────┼──────────────────────────────┼──────────────────────────────┼──────────────────────────────┤\n",
      "│ \u001b[38;5;226m   \\  /\u001b[0m       Partly Cloudy  │               Cloudy         │ \u001b[38;5;226m _`/\"\"\u001b[38;5;240;1m.-.    \u001b[0m Moderate or he…│ \u001b[38;5;250m     .-.     \u001b[0m Light rain     │\n",
      "│ \u001b[38;5;226m _ /\"\"\u001b[38;5;250m.-.    \u001b[0m \u001b[38;5;154m60\u001b[0m °F\u001b[0m          │ \u001b[38;5;250m     .--.    \u001b[0m \u001b[38;5;154m62\u001b[0m °F\u001b[0m          │ \u001b[38;5;226m  ,\\_\u001b[38;5;240;1m(   ).  \u001b[0m \u001b[38;5;118m+57\u001b[0m(\u001b[38;5;118m55\u001b[0m) °F\u001b[0m     │ \u001b[38;5;250m    (   ).   \u001b[0m \u001b[38;5;118m+55\u001b[0m(\u001b[38;5;082m53\u001b[0m) °F\u001b[0m     │\n",
      "│ \u001b[38;5;226m   \\_\u001b[38;5;250m(   ).  \u001b[0m \u001b[1m↗\u001b[0m \u001b[38;5;214m13\u001b[0m-\u001b[38;5;208m16\u001b[0m mph\u001b[0m    │ \u001b[38;5;250m  .-(    ).  \u001b[0m \u001b[1m↗\u001b[0m \u001b[38;5;214m13\u001b[0m-\u001b[38;5;208m14\u001b[0m mph\u001b[0m    │ \u001b[38;5;226m   /\u001b[38;5;240;1m(___(__) \u001b[0m \u001b[1m↙\u001b[0m \u001b[38;5;190m6\u001b[0m-\u001b[38;5;226m8\u001b[0m mph\u001b[0m      │ \u001b[38;5;250m   (___(__)  \u001b[0m \u001b[1m↙\u001b[0m \u001b[38;5;226m9\u001b[0m-\u001b[38;5;220m11\u001b[0m mph\u001b[0m     │\n",
      "│ \u001b[38;5;226m   /\u001b[38;5;250m(___(__) \u001b[0m 6 mi\u001b[0m           │ \u001b[38;5;250m (___.__)__) \u001b[0m 6 mi\u001b[0m           │ \u001b[38;5;21;1m   ‚‘‚‘‚‘‚‘  \u001b[0m 6 mi\u001b[0m           │ \u001b[38;5;111m    ‘ ‘ ‘ ‘  \u001b[0m 1 mi\u001b[0m           │\n",
      "│               0.0 in | 0%\u001b[0m    │               0.0 in | 0%\u001b[0m    │ \u001b[38;5;21;1m   ‚’‚’‚’‚’  \u001b[0m 0.1 in | 100%\u001b[0m  │ \u001b[38;5;111m   ‘ ‘ ‘ ‘   \u001b[0m 0.1 in | 100%\u001b[0m  │\n",
      "└──────────────────────────────┴──────────────────────────────┴──────────────────────────────┴──────────────────────────────┘\n",
      "                                                       ┌─────────────┐                                                       \n",
      "┌──────────────────────────────┬───────────────────────┤  Sun 30 Mar ├───────────────────────┬──────────────────────────────┐\n",
      "│            Morning           │             Noon      └──────┬──────┘     Evening           │             Night            │\n",
      "├──────────────────────────────┼──────────────────────────────┼──────────────────────────────┼──────────────────────────────┤\n",
      "│ \u001b[38;5;226m _`/\"\"\u001b[38;5;250m.-.    \u001b[0m Thundery outbr…│               Overcast       │ \u001b[38;5;226m _`/\"\"\u001b[38;5;240;1m.-.    \u001b[0m Heavy rain at …│ \u001b[38;5;226m   \\  /\u001b[0m       Partly Cloudy  │\n",
      "│ \u001b[38;5;226m  ,\\_\u001b[38;5;250m(   ).  \u001b[0m \u001b[38;5;154m60\u001b[0m °F\u001b[0m          │ \u001b[38;5;240;1m     .--.    \u001b[0m \u001b[38;5;154m60\u001b[0m °F\u001b[0m          │ \u001b[38;5;226m  ,\\_\u001b[38;5;240;1m(   ).  \u001b[0m \u001b[38;5;154m60\u001b[0m °F\u001b[0m          │ \u001b[38;5;226m _ /\"\"\u001b[38;5;250m.-.    \u001b[0m \u001b[38;5;082m+51\u001b[0m(\u001b[38;5;046m48\u001b[0m) °F\u001b[0m     │\n",
      "│ \u001b[38;5;226m   /\u001b[38;5;250m(___(__) \u001b[0m \u001b[1m↑\u001b[0m \u001b[38;5;220m11\u001b[0m-\u001b[38;5;202m19\u001b[0m mph\u001b[0m    │ \u001b[38;5;240;1m  .-(    ).  \u001b[0m \u001b[1m↑\u001b[0m \u001b[38;5;214m14\u001b[0m-\u001b[38;5;196m22\u001b[0m mph\u001b[0m    │ \u001b[38;5;226m   /\u001b[38;5;240;1m(___(__) \u001b[0m \u001b[1m↗\u001b[0m \u001b[38;5;220m11\u001b[0m-\u001b[38;5;196m22\u001b[0m mph\u001b[0m    │ \u001b[38;5;226m   \\_\u001b[38;5;250m(   ).  \u001b[0m \u001b[1m↘\u001b[0m \u001b[38;5;154m4\u001b[0m-\u001b[38;5;190m6\u001b[0m mph\u001b[0m      │\n",
      "│ \u001b[38;5;228;5m    ⚡\u001b[38;5;111;25m‘‘\u001b[38;5;228;5m⚡\u001b[38;5;111;25m‘‘ \u001b[0m 5 mi\u001b[0m           │ \u001b[38;5;240;1m (___.__)__) \u001b[0m 6 mi\u001b[0m           │ \u001b[38;5;21;1m   ‚‘‚‘‚‘‚‘  \u001b[0m 6 mi\u001b[0m           │ \u001b[38;5;226m   /\u001b[38;5;250m(___(__) \u001b[0m 6 mi\u001b[0m           │\n",
      "│ \u001b[38;5;111m    ‘ ‘ ‘ ‘  \u001b[0m 0.0 in | 0%\u001b[0m    │               0.0 in | 0%\u001b[0m    │ \u001b[38;5;21;1m   ‚’‚’‚’‚’  \u001b[0m 0.1 in | 100%\u001b[0m  │               0.0 in | 0%\u001b[0m    │\n",
      "└──────────────────────────────┴──────────────────────────────┴──────────────────────────────┴──────────────────────────────┘\n",
      "Location: O'Hare International Airport, 10000, O'Hare Commercial Arrivals, Chicago, Illinois, 60666, United States of America [41.978131,-87.9095288486136]\n",
      "\n",
      "Follow \u001b[46m\u001b[30m@igor_chubin\u001b[0m for wttr.in updates\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "if location != \"user_error\":\n",
    "    url = f\"https://wttr.in/{location}\"\n",
    "    print(f\"Getting weather for {location}\")\n",
    "    wttr_response = requests.get(url)\n",
    "    if wttr_response.status_code == 200:\n",
    "        print(wttr_response.text)\n",
    "    else:\n",
    "        print(wttr_response.status_code)\n",
    "else:\n",
    "    print(\"Must ask for weather at city, location, or airport.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
