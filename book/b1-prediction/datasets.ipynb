{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "> Collecting a dataset is typically the most difficult, time-consuming, and expensive part of any edge AI project. It’s also the most likely place you will make terrible, hard-to-detect mistakes that can doom your project to failure. ~ [*AI at the Edge*](https://learning.oreilly.com/library/view/ai-at-the/9781098120191/ch07.html#:-:text=Collecting%20a%20dataset,project%20to%20failure.)\n",
    "\n",
    "## Pre-Reading\n",
    "\n",
    "- (Critical) Chollet, [2.1 A first look at a neural network](https://learning.oreilly.com/library/view/deep-learning-with/9781617296864/Text/02.htm#:-:text=2.1%20A%20first%20look%20at%20a%20neural%20network) and [2.2 Data representations for neural networks](https://learning.oreilly.com/library/view/deep-learning-with/9781617296864/Text/02.htm#:-:text=2.2%20Data%20representations%20,for%20neural%20networks)\n",
    "- (Bonus) Stevens, [*Deep Learning with PyTorch*, 5.5.3 Training, validation, and overfitting](https://learning.oreilly.com/library/view/deep-learning-with/9781617295263/Text/05.xhtml#:-:text=5.5.3%20Training%2C%20validation%2C%20and%20overfitting)\n",
    "\n",
    "\n",
    "### Objectives\n",
    "\n",
    "1. Describe an ideal dataset for machine learning.\n",
    "2. Understand how samples in a dataset can be represented as n-rank tensors.\n",
    "3. Distinguish between the functions of train, test, and validation set splits.\n",
    "4. List some popular platforms for dataset hosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Time\n",
    "\n",
    "![The Prediction Machine](../img/my_prediction_machine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What makes a dataset\n",
    "\n",
    "See Situnayake and Plunkett's [*AI at the Edge: Chapter 7 How to Build a Dataset](https://learning.oreilly.com/library/view/ai-at-the/9781098120191/ch07.html) for more info.\n",
    "\n",
    "A dataset contains: **samples**, each of which may contain many **features** of various data types.\n",
    "Many datasets also contain *labels* which are a special kind of feature that indicate the desired output of a trained model for a given sample.\n",
    "\n",
    "It is common for datasets to have *metadata* about the dataset itself, such as how the data was collected.\n",
    "\n",
    "### An Ideal Dataset\n",
    "\n",
    "An ideal dataset has the following properties:\n",
    "\n",
    "- Relevant to the problem you are trying to solve\n",
    "- Representative of all the various conditions your model might encounter in its real-world task\n",
    "- Balanced quantities of those various conditions\n",
    "- Reliable, containing as few errors as possible\n",
    "- Formatted in a way that it can be easily employed\n",
    "- Documented sufficiently for you to answer the above questions\n",
    "- Appropriately sized to address your problem\n",
    "\n",
    "#### Domain Expertise\n",
    "\n",
    "```{tip}\n",
    "Even with the perfect dataset, it is crucial to consult **domain experts** to help you understand the data and the **context**.\n",
    "\n",
    "Without this, you will likely misunderstand the problem and get undesired outcomes.\n",
    "```\n",
    "\n",
    "Domain experts can help you make sure that your dataset is representative and reliable.\n",
    "\n",
    "### Ethics\n",
    "\n",
    "> The quality of your dataset will shape your application’s social consequences more than any other factor. No matter how carefully you have worked to investigate the ethical issues around your project, and to design an application that delivers benefit while being safe, the limitations of your dataset dictate your ability to understand and avoid unintentional harm. ~ *AI at the Edge*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing Datasets as Tensors\n",
    "\n",
    "Machine learning datasets cover myriad use-cases and come in numerous forms.\n",
    "For example, here is a screenshot of the trending datasets on [Kaggle](https://www.kaggle.com/datasets), an extremely popular platform for hosting ML datasets and models.\n",
    "\n",
    "![Trending Kaggle datasets](../img/kaggle_datasets.png)\n",
    "\n",
    "Yet, to be useful in training a neural network they must be converted into a regular form upon which we can do math.\n",
    "\n",
    "**All datasets for machine learning must be represented as tensors.**\n",
    "*WHY???* Because we need to do math on them! (Spoiler alert: Especially dot-products, which GPUs are really good at!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Tensor?\n",
    "\n",
    "> At its core, a tensor is a container for data—usually numerical data. So, it’s a container for numbers. You may be already familiar with matrices, which are rank-2 tensors: tensors are a generalization of matrices to an arbitrary number of dimensions (note that in the context of tensors, a dimension is often called an axis). ~ *Deep Learning With Python*\n",
    "\n",
    "A *tensor* is simply an array of numbers arranged along one or more axes.\n",
    "\n",
    "![Dimensions of a Tensor (JavaPoint)](https://images.javatpoint.com/tutorial/pytorch/images/pytorch-tensors.png)\n",
    "\n",
    "For a helpful visual breakdown, see Dan Fleisch's [What is a Tensor? (YouTube)](https://youtu.be/f5liqUk0ZTw?)\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube-nocookie.com/embed/f5liqUk0ZTw?si=02YMXVF0YpBT1FMF&amp;start=375\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: 3\n",
      "Shape: (3, 3, 5)\n",
      "Data Type: int64\n",
      "Value of item at position (1, 2, 3): 2016\n"
     ]
    }
   ],
   "source": [
    "# Create a Rank-3 Tensor in Numpy\n",
    "import numpy as np\n",
    "\n",
    "x = np.array(\n",
    "    [\n",
    "        [[5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [37, 8, 58, 36, 11]],\n",
    "        [[5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [37, 8, 0, 2016, 0]],\n",
    "        [[5, 78, 2, 34, 0], [6, 79, 3, 35, 1], [37, 8, 58, 36, 11]],\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Dimensions: {x.ndim}\")\n",
    "print(f\"Shape: {x.shape}\")\n",
    "print(f\"Data Type: {x.dtype}\")\n",
    "\n",
    "print(f\"Value of item at position (1, 2, 3): {x[1, 2, 3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World examples of data tensors\n",
    "\n",
    "See [Cholet, *Deep Learning with Python, 2nd Ed*: 2.2.8-12](https://learning.oreilly.com/library/view/deep-learning-with/9781617296864/Text/02.htm#:-:text=2.2.8%20Real-world%20examples%20of%20data%20tensors)\n",
    "\n",
    "In ML datasets axis 0 is the *sample axis*.\n",
    "Along this axis are *batches* of multiple samples. Processing data in batches speeds up learning and inference.\n",
    "\n",
    "The other axes of a tensor are specific to the dataset, but typically fall into:\n",
    "\n",
    "> - *Vector data*: Rank-2 tensors of shape (samples, features), where each sample is a vector of numerical attributes (“features”)\n",
    "> - *Timeseries data or sequence data*: Rank-3 tensors of shape (samples, timesteps, features), where each sample is a sequence (of length timesteps) of feature vectors\n",
    "> - *Images*: Rank-4 tensors of shape (samples, height, width, channels), where each sample is a 2D grid of pixels, and each pixel is represented by a vector of values (“channels”)\n",
    "> - *Video*: Rank-5 tensors of shape (samples, frames, height, width, channels), where each sample is a sequence (of length frames) of images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test, & Validation Sets\n",
    "\n",
    "We need our trained models to be able to *generalize* to samples from the world that they've never seen before.\n",
    "\n",
    "See Stevens, [*Deep Learning with PyTorch*, 5.5.3 Training, validation, and overfitting](https://learning.oreilly.com/library/view/deep-learning-with/9781617295263/Text/05.xhtml#:-:text=5.5.3%20Training%2C%20validation%2C%20and%20overfitting)\n",
    "\n",
    "> - **Training split** is used directly to develop an algorithm, typically by training a machine learning model.\n",
    "> - **Validation split** is used to evaluate the model *during* iterative training\n",
    "> - **Testing split** is kept aside until the very end of training. It is used in a final pass to ensure that the model is able to perform well on data that it has never been exposed to before\n",
    ">\n",
    "> [*AI at the Edge*](https://learning.oreilly.com/library/view/ai-at-the/9781098120191/ch07.html#:-:text=Splitting%20Your%20Data)\n",
    "\n",
    "### Train and Test\n",
    "\n",
    "Some datasets are pre-split into test and train.\n",
    "\n",
    "Some datasets you manually make the split yourself.\n",
    "\n",
    "For competitions the test set may be kept secret by the host and they will run it against your model without you ever seeing it!\n",
    "\n",
    "- The **training set** is the part of the dataset we use to *optimize* our model.\n",
    "- The **test set** is the part of the dataset we use to *evaluate* our model.\n",
    "\n",
    "We want as much data as possible to train against, *but* we need samples that our model has never seen to evaluate against. Just like how you practice homework problems but an exam has similar problems that you have never seen, rather than identical problems to the homework.\n",
    "\n",
    "For a standard dataset an **80/20 train/test split is typical**.\n",
    "For smaller datasets you may want to reserve more for training while for larger datasets you can devote more to test.\n",
    "\n",
    "#### Evaluate\n",
    "\n",
    "For labeled data, Keras has the builtin evaluate method:\n",
    "\n",
    "```python\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"test_acc: {test_acc}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "What if we were making a model to fit a curve of points in our training set and we came up with two options, shown in blue vs. red below:\n",
    "\n",
    "![Stevens: Overfitting](../img/CH05_F14_Stevens2_overfittring.png)\n",
    "\n",
    "Clearly we should go with the blue model because the red one just looks... wrong.\n",
    "The second model is *overfitting*, \n",
    "\n",
    "> **Overfitting** a model happens when the model’s performance continues to improve on the training set but degrades on the validation set. This is usually due to the model not generalizing, and instead memorizing the desired outputs for the training set. ~ Stevens, *Deep Learning with PyTorch*\n",
    "\n",
    "We can use a *validation set* to monitor overfitting.\n",
    "The figure below shows training loss as a blue solid line and validation loss as a red dotted line.\n",
    "\n",
    "![Stevens: Fig. 14, overfitting scenarios](../img/CH05_F16_Stevens2_train_val_loss.png)\n",
    "\n",
    "\n",
    "When we are training a model we want to see the loss steadily decreasing as we iterate.\n",
    "\n",
    "- **A** no learning is happening\n",
    "- **B** overfitting because validation loss is increasing while training loss is decreasing\n",
    "- **C** ideal, both decreasing\n",
    "- **D** acceptable, because both decreasing although some overfitting still occurring\n",
    "\n",
    "For a standard dataset, taking **20% of the already split train set for validation is typical**.\n",
    "This means an approximate train/validate/test spilt of 74/16/20\n",
    "It is critical that all three sets evenly represent all classes.\n",
    "\n",
    "#### Using Validation During Training\n",
    "\n",
    "In Keras the [`fit()` method](https://keras.io/api/models/model_training_apis/#fit-method) includes the following optional parameters:\n",
    "\n",
    "- `validation_split` is a fraction of the training data to use for validation and provide its evaluation at the end of each epoch.\n",
    "- `validation_data` allows you to pass in a pre-split validation set.\n",
    "\n",
    "```python\n",
    "#  Note that validation_split just takes samples from the end of the dataset, so you need to make sure these are representative.\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platforms for Hosting Datasets\n",
    "\n",
    "We will see that scikit-learn, TensorFlow, and PyTorch all have datasets included in their libraries.\n",
    "But real datasets are very large and hosted separately.\n",
    "\n",
    "Popular platforms for hosting datasets:\n",
    "\n",
    "- [Kaggle](https://www.kaggle.com/datasets)\n",
    "- [🤗 HuggingFace](https://huggingface.co/datasets)\n",
    "- [data.gov](https://data.gov/)\n",
    "- Or see the [GeeksforGeeks \"Top 10 Dataset Websites](https://www.geeksforgeeks.org/top-dataset-websites/)\n",
    "\n",
    "### Example: Refined Web\n",
    "\n",
    "The Technology Innovation Institute, a research institution funded by the Abu Dhabi government, created and released the [Refined Web Dataset](https://huggingface.co/datasets/tiiuae/falcon-refinedweb). TII uses Refined Web to train its Falcon family of large language models, which topped the LLM leader boards at one point last year.\n",
    "\n",
    "> RefinedWeb is built through stringent filtering and large-scale deduplication of CommonCrawl; we found models trained on RefinedWeb to achieve performance in-line or better than models trained on curated datasets, while only relying on web data.\n",
    ">\n",
    "> This public extract should contain 500-650GT depending on the tokenizer you use, and can be enhanced with the curated corpora of your choosing. This public extract is about ~500GB to download, requiring **2.8TB of local storage once unpacked**.\n",
    "\n",
    "```python\n",
    "# Don't run this unless you have time snd space for 500GB to download...\n",
    "from datasets import load_dataset\n",
    "rw = load_dataset(\"tiiuae/falcon-refinedweb\")\n",
    "```\n",
    "\n",
    "See [tiiuae/falcon-refinedweb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) for more!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
