{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization\n",
    "\n",
    "## Pre-reading\n",
    "\n",
    "Watch the video **DeepLearningAI-Quantization_Fundamentals-Handling_Big_Models** posted in Teams\n",
    "\n",
    "### Objectives\n",
    "\n",
    "1. Describe different data types supported by ARM, PyTorch, and TensorFlow Lite.\n",
    "2. Quantize data into different data types.\n",
    "3. Assess the impact on memory usage of quantization.\n",
    "\n",
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types and Sizes\n",
    "\n",
    "*Some of this content is from [DeepLearning AI \"Quantization Fundamentals\"](https://learn.deeplearning.ai/courses/quantization-fundamentals/lesson/dig9h/data-types-and-sizes)\n",
    "\n",
    "PyTorch and TensorFlow both support various data types. Many - but not all - of these are familiar to you from your experience with C programming.\n",
    "\n",
    "This course has thusfar focussed on TensorFlow, but PyTorch has better support for this sort of stuff, particularly as HuggingFace and the rest of the community continues to favor PyTorch over TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer Types\n",
    "\n",
    "Unsigned go from $[0, 2^N -1]$\n",
    "\n",
    "Signed are two's complement and go from $[-2^{N-1}, 2^{N-1}-1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information of `8-bit unsigned integer`\n",
    "torch.iinfo(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information of `8-bit (signed) integer`\n",
    "torch.iinfo(torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Information of `16-bit (signed) integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Information of `32-bit (signed) integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Information of `64-bit (signed) integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating Point Types\n",
    "\n",
    "The decimal \"floats\", and the number is expressed as a base and exponent.\n",
    "\n",
    "IEEE 754 single-precision **FP32** has:\n",
    "\n",
    "- 1 sign bit\n",
    "- 8 exponent bits\n",
    "- 23 fraction bits\n",
    "\n",
    "But there are other formats!\n",
    "\n",
    "![floating point formats](https://frankdenneman.nl/wp-content/uploads/2022/07/FP16-FP32-BFfloat16-50dpi.png)\n",
    "\n",
    "Python defaults to FP64 for float data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information of `64-bit floating point`\n",
    "torch.finfo(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information of `32-bit floating point`\n",
    "torch.finfo(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Information of `16-bit floating point`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default, python stores float data in fp64\n",
    "value = 1 / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format(value, \".60f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_fp64 = torch.tensor(value, dtype=torch.float64)\n",
    "tensor_fp32 = torch.tensor(value, dtype=torch.float32)\n",
    "tensor_fp16 = torch.tensor(value, dtype=torch.float16)\n",
    "tensor_bf16 = torch.tensor(value, dtype=torch.bfloat16)\n",
    "\n",
    "print(f\"fp64 tensor: {format(tensor_fp64.item(), '.60f')}\")\n",
    "print(f\"fp32 tensor: {format(tensor_fp32.item(), '.60f')}\")\n",
    "print(f\"fp16 tensor: {format(tensor_fp16.item(), '.60f')}\")\n",
    "print(f\"bf16 tensor: {format(tensor_bf16.item(), '.60f')}\")  # More on this below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bfloat16\n",
    "\n",
    "\n",
    "Developed by Google Brian, **bfloat16** has approximately the same dynamic range as 32-bit float, but only has 8-bit precision instead of float32's 24-bits of precision.\n",
    "\n",
    "Most machine learning applications do not require single-precision, but simply casting to FP16 sacrifices dynamic range.\n",
    "The smaller size of bfloat16 numbers allow for more efficient memory usage and calculation speed compared to float32.\n",
    "\n",
    "See [bfloat16 Wikipedia](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format) for more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information of `16-bit brain floating point (bfloat16)`\n",
    "torch.finfo(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### bfloat16 on ARM processors\n",
    "\n",
    "> Recent Arm processors support the BFloat16 (BF16) number format in PyTorch. BFloat16 provides improved performance and smaller memory footprint with the same dynamic range. You might experience a drop in model inference accuracy with BFloat16, but the impact is acceptable for the majority of applications. ~ [ARM Learn: PyTorch](https://learn.arm.com/install-guides/pytorch/)\n",
    "\n",
    "To check if your system includes BFloat16, use the `lscpu` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will print flags if your processor supports BFloat16\n",
    "# If result is blank you do not have a processor with BFloat16.\n",
    "!lscpu | grep bf16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Point Types\n",
    "\n",
    "We previously discussed the CMSIS support in ARM processors and how they operate on the Q number format.\n",
    "\n",
    "- A Q number has a sign bit followed by a fixed number of bits to represent the integer value.\n",
    "- The remaining bits represent the fractional part of the number. \n",
    "- Signed Q numbers are stored as two‚Äôs complement values.\n",
    "- A Q number is typically referred to by the number of fractional bits it uses so Q7 has 7 fractional places.\n",
    "\n",
    "The fractional bits are represented as negative powers of two.\n",
    "\n",
    "![Fixed Point Representation](https://media.geeksforgeeks.org/wp-content/uploads/20220728220542/geeksforgeeksbinarypointrepresentation.png)\n",
    "\n",
    "The CMSIS DSP library functions are designed to take input values in the range `[-1, 1)`, so the fractional section takes all the bits of the data type minus one bit that is used as a sign bit.\n",
    "\n",
    "| CMSIS DSP Type Def | Q number | C Type |\n",
    "|--------------------|----------|--------|\n",
    "| Q31_t              | Q31      | Int 32 |\n",
    "| Q15_t              | Q15      | Int 16 |\n",
    "| Q7_t               | Q7       | Int 8  |\n",
    "\n",
    "```{tip}\n",
    "Many neural networks expect inputs in the normalized range `[-1, 1)`!\n",
    "```\n",
    "\n",
    "- Neither TensorFlow or PyTorch directly support fixed-point numbers\n",
    "- LiteRT uses floating-point numbers because they are more widely supported and almost as fast.\n",
    "- Ultra-optimized libraries, such as some of those from EdgeImpulse *do* use these functions.\n",
    "\n",
    "#### Computation with Q Numbers\n",
    "\n",
    "> Like all binary number representations, fixed-point numbers are just a collection of bits. There is no way of knowing the existence of the binary point except through agreement of those people interpreting the number. ~ *Digital Design and Computer Architecture, ARM Edition*\n",
    "\n",
    "Because the placement of the decimal is just an agreement, **`int8` and `Q7` can use much of the same hardware!**\n",
    "\n",
    "The biggest difference is how overflows are handled for integers versus handling saturations for Q numbers.\n",
    "\n",
    "Let's ask [ChatGPT for an example](https://chatgpt.com/share/67aee249-bab8-8003-b6ef-f9e3456aef08) of multiply-accumulate with the two data types! And then have me adjust it...\n",
    "\n",
    "##### int8 Example\n",
    "\n",
    "Input:\n",
    "- A = -50 (int8) ‚Üí `11001110`\n",
    "- B = 30 (int8) ‚Üí `00011110`\n",
    "- Accumulator (int16) = 500 ‚Üí `00000001 11110100`\n",
    "\n",
    "1. Multiply: $-50 * 30 = -1500$ ‚Üí `11111010 00100100` (16-bit two's complement)\n",
    "2. Accumulate: $500 + -1500 = -1000$ ‚Üí `11111100 00001000`\n",
    "\n",
    "##### Q7 Example\n",
    "\n",
    "Input:\n",
    "- A = -0.390625 (Q7) ‚Üí `11001110` (same binary as int8)\n",
    "- B = 0.234375 (Q7) ‚Üí `00011110` (same binary as int8)\n",
    "- Accumulator = 3.90625 (Q7 but with more integer bits) ‚Üí `00000001 11110100` (same binary as int16)\n",
    "\n",
    "1. Multiply: $-0.390625 * 0.234375 = -0.091552734375$ ‚Üí `11111010 00100100` (still the same binary, but is -11.71875 in Q7!)\n",
    "2. Q7 format shift adjustment: $>>7$ ‚Üí `11110100` = -0.09375 (*close* to -0.09155)\n",
    "3. Accumulate: $3.90625 + -0.09375 = 3.8125$ ‚Üí `00000011 11101000` (why is this so different than the int8?)\n",
    "\n",
    "**Conclusion:** The final results are very different, but the thing to keep in mind is the *scaling* of Q7.\n",
    "The range of int8 is $2^7=128$ times that of Q7, so we can compare the two by multiplying Q7 by 128.\n",
    "\n",
    "In this case, the int8 accumulator changed by a factor of $\\frac{-1000-500}{500} = -3.0)$\n",
    "\n",
    "Meanwhile, the Q7 accumulator changed by a factor of $\\frac{488-500}{500} = -0.024)$, which if you scale by 128 is $3.072$, very close to the int8 relative change!\n",
    "\n",
    "You can play around with this using [chummersone Q-format converter](https://chummersone.github.io/qformat.html#converter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion between types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downcasting\n",
    "\n",
    "*Downcasting* converts a type of higher precision to lower precision. It results in a loss of data.\n",
    "\n",
    "The [`torch.Tensor.to()`](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html#torch.Tensor.to) method:\n",
    "\n",
    "- Allows you to convert a tensor to a specified [`dtype`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype) (such as `uint8` or `bfloat16`)\n",
    "- Allows you to send a tensor to a specified [`device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device) (such as `cpu` or `cuda`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random pytorch tensor: float32, size=1000\n",
    "tensor_fp32 = torch.rand(1000, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downcast the tensor to bfloat16 using the \"to\" method\n",
    "tensor_fp32_to_bf16 = tensor_fp32.to(dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First five elements torch.float32  {tensor_fp32[:5]}\")\n",
    "print(f\"First five elements torch.bfloat16 {tensor_fp32_to_bf16[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_fp32 x tensor_fp32\n",
    "torch.dot(tensor_fp32, tensor_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_fp32_to_bf16 x tensor_fp32_to_bf16\n",
    "torch.dot(tensor_fp32_to_bf16, tensor_fp32_to_bf16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downcast a Model\n",
    "\n",
    "We will follow [an example](https://learn.deeplearning.ai/courses/quantization-fundamentals/lesson/rq9jf/loading-models-by-data-type) to load [Salesforce/blip-image-captioning-base](https://huggingface.co/Salesforce/blip-image-captioning-base) from HuggingFace. It is a model that captions images\n",
    "\n",
    "We will then inspect the model's **memory footprint** in both its default float32 datatype and downcast to bfloat16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers requests pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipForConditionalGeneration\n",
    "\n",
    "model_name = \"Salesforce/blip-image-captioning-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model with default dtype (float32)\n",
    "model = BlipForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_mem_footprint = model.get_memory_footprint()\n",
    "print(\"Footprint of the fp32 model in MBs: \", fp32_mem_footprint / 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model in bfloat16 format\n",
    "model_bf16 = BlipForConditionalGeneration.from_pretrained(\n",
    "    model_name, torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf16_mem_footprint = model_bf16.get_memory_footprint()\n",
    "print(\"Footprint of the bf16 model in MBs: \", bf16_mem_footprint / 1e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare the relative size of the two formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the relative difference of the two formats\n",
    "relative_diff = bf16_mem_footprint / fp32_mem_footprint\n",
    "print(f\"Relative diff: {relative_diff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't worry about understanding this code\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import BlipProcessor\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def get_generation(model, processor, image, dtype):\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(dtype)\n",
    "    out = model.generate(**inputs)\n",
    "    return processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def load_image(img_url):\n",
    "    image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "img_url = \"https://storage.googleapis.com/\\\n",
    "sfr-vision-language-research/BLIP/demo.jpg\"\n",
    "\n",
    "image = load_image(img_url)\n",
    "display(image.resize((500, 350)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result from original model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fp32 = get_generation(model, processor, image, torch.float32)\n",
    "print(\"fp32 Model Results:\\n\", results_fp32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result from downcast model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bf16 = get_generation(model_bf16, processor, image, torch.bfloat16)\n",
    "print(\"bf16 Model Results:\\n\", results_bf16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the original model says \"a woman sitting on the beach with **her** dog\" vs. \"a woman sitting on the beach with **a** dog.\" That tiny difference in inference is the result of accumulated errors through the large number of downcast parameters throughout the model. Nothing is free!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Quantization\n",
    "\n",
    "> Quantization is the process of mapping a large set to a small set of values.\n",
    "\n",
    "![FP32 -> INT8](https://developer-blogs.nvidia.com/wp-content/uploads/2021/07/qat-training-precision.png)\n",
    "\n",
    "- Most commonly converts float32 to int8.\n",
    "- Quantization can improve performance and reduce power consumption, but it may reduce accuracy.\n",
    "- Post-training quantization (PTQ) is performed after a model has been trained, while quantization-aware training (QAT) is performed during training.\n",
    "- QAT typically produces better accuracy than PTQ, but it is computationally more expensive.\n",
    "\n",
    "Linear quantization is straightforward and effective.\n",
    "\n",
    "$$\n",
    "r = s(q-z)\n",
    "$$\n",
    "\n",
    "Where $r$ is the original FP32 value; $s$ is the scale; $q$ is the quantized INT8 value; $z$ is the zero point.\n",
    "\n",
    "```{figure} ../img/linear_quantization_deeplearningai.png\n",
    "Linear mapping from FP32 to INT8 with **scale** and **zero point**.\n",
    "From [Quantization Fundamentals with Hugging Face](https://learn.deeplearning.ai/courses/quantization-fundamentals/lesson/ugesv/quantization-theory)\n",
    "```\n",
    "\n",
    "Scale and zero are found by analyzing the extreme values.\n",
    "\n",
    "$$\n",
    "r_{min} = s(q_{min}-z)\n",
    "$$\n",
    "$$\n",
    "r_{max} = s(q_{max}-z)\n",
    "$$\n",
    "$$\n",
    "s = (r_{max} - r_{min})/(q_{max} - q_{min})\n",
    "$$\n",
    "$$\n",
    "z = \\text{int}(\\text{round}(q_{min}-r_{min}/s))\n",
    "$$\n",
    "\n",
    "As a result, the *larger* the dynamic range you are trying to quantize, the *poorer* the precision will be!\n",
    "\n",
    "![symmetric dynamic range](https://developer-blogs.nvidia.com/wp-content/uploads/2021/07/8-bit-signed-integer-quantization.png)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization in LiteRT\n",
    "\n",
    "The [LiteRT 8-bit quantization specification](https://ai.google.dev/edge/litert/models/quantization_spec) contains more detail than what we really need. But two good things to know:\n",
    "\n",
    "1. LiteRT uses *signed* 8-bit integer quantization according to the same formula as the one given above.\n",
    "2. Activation functions are *asymmetric*, meaning the zero-point can be anywhere within the `int8` range of `[-128, 127]`. However, model weights are *symmetric*, which forces the zero point equal to 0, which reduces the computational cost by eliminating a multiply.\n",
    "\n",
    "[Post-training quantization](https://ai.google.dev/edge/litert/models/post_training_quantization) comes in several varieties.\n",
    "\n",
    "|          Technique         |           Benefits           |             Hardware            |\n",
    "|:--------------------------:|:----------------------------:|:-------------------------------:|\n",
    "| Dynamic range quantization | 4x smaller, 2x-3x speedup    | CPU                             |\n",
    "| Full integer quantization  | 4x smaller, 3x+ speedup      | CPU, Edge TPU, Microcontrollers |\n",
    "| Float16 quantization       | 2x smaller, GPU acceleration | CPU, GPU                        |\n",
    "\n",
    "![LiteRT quantization flow](https://ai.google.dev/edge/litert/images/models/optimization.jpg)\n",
    "\n",
    "In this course we will focus on  **Dynamic range quantization**, which is the default method when converting a TF Lite model.\n",
    "\n",
    "> Dynamic range quantization provides reduced memory usage and faster computation without you having to provide a representative dataset for calibration. This type of quantization, statically **quantizes only the weights** from floating point to integer at conversion time, which provides 8-bits of precision:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT] # Dynamic range quantization to int8\n",
    "tflite_quant_model = converter.convert()\n",
    "```\n",
    "\n",
    "> To further reduce latency during inference, **\"dynamic-range\" operators dynamically quantize activations** based on their range to 8-bits and perform computations with 8-bit weights and activations. This optimization provides latencies close to fully fixed-point inferences. **However, the outputs are still stored using floating point so the increased speed of dynamic-range ops is less than a full fixed-point computation.**\n",
    "\n",
    "When we run *inference* on a Raspberry Pi, for example, we should strongly consider doing this! [foreshadowing üê±üê∂](../b3-devboard/lab-cat-dog.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
