{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BpWg3hJENz0"
   },
   "source": [
    "# ICE 4: GPU Acceleration\n",
    "\n",
    "Let's check out some different PyTorch arithmetic on the CPU vs. GPU!\n",
    "\n",
    "We'll present the same code for both TensorFlow and Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUsKwMKeSSR5"
   },
   "source": [
    "First, import and check the version.\n",
    "\n",
    "We will also make sure we can access the CUDA GPU.\n",
    "This should always be your first step!\n",
    "\n",
    "We'll also set a manual_seed for the random operations. While this isn't strictly necessary for this experiment, it's good practice as it aids with reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ocKjJH0EYb9",
    "outputId": "2a38f8b2-2df1-46d3-a8f9-edac4c138d08"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "\n",
    "# Help with reproducibility of test\n",
    "torch.manual_seed(2016)\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise OSError(\"ERROR: No GPU found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "\n",
    "# Help with reproducibility of test\n",
    "tf.random.set_seed(2016)\n",
    "\n",
    "# Make sure we can access the GPU\n",
    "print(\"Physical Devices Available:\\n\", tf.config.list_physical_devices())\n",
    "if not tf.config.list_physical_devices(\"GPU\"):\n",
    "    raise OSError(\"ERROR: No GPU found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JICBen1CSoPh"
   },
   "source": [
    "## Dot Product\n",
    "\n",
    "Dot products are **extremely** common tensor operations. They are used deep neural networks and linear algebra applications.\n",
    "\n",
    "A dot product is essentially just a bunch of multiplications and additions.\n",
    "\n",
    "- PyTorch provides the [`torch.tensordot()`](https://pytorch.org/docs/stable/generated/torch.tensordot.html) method.\n",
    "- TensorFlow provides the [`tf.tensordot()`](https://www.tensorflow.org/api_docs/python/tf/tensordot) method.\n",
    "\n",
    "First, let's define two methods to compute the dot product. One will take place on the CPU and the other on the GPU.\n",
    "\n",
    "### CPU Timing\n",
    "\n",
    "The CPU method is trivial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1eioHVdEJVg"
   },
   "outputs": [],
   "source": [
    "# Compute the tensor dot product on CPU\n",
    "def torch_cpu_dot_product(a, b):\n",
    "    return torch.tensordot(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the tensor dot product on CPU\n",
    "def tf_cpu_dot_product(a, b):\n",
    "    with tf.device(\"/CPU:0\"):\n",
    "        product = tf.tensordot(a, b, axes=2)\n",
    "    return product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Timing\n",
    "\n",
    "For **PyTorch** the GPU method has a bit more two it. We must:\n",
    "\n",
    "1. Send the tensors to the GPU for computation. We call torch.to() on the tensor to send it to a particular device\n",
    "2. Wait for the GPU to synchronize. According to the docs, GPU ops take place asynchronously so you need to use synchronize for precise timing.\n",
    "\n",
    "For **TensorFlow** the `tf.device` makes it a bit simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the tensor to GPU then compute dot product\n",
    "# synchronize() required for timing accuracy, see:\n",
    "# https://pytorch.org/docs/stable/notes/cuda.html#asynchronous-execution\n",
    "def torch_gpu_dot_product(a, b):\n",
    "    a_gpu = a.to(\"cuda\")\n",
    "    b_gpu = b.to(\"cuda\")\n",
    "    product = torch.tensordot(a_gpu, b_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    return product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_gpu_dot_product(a, b):\n",
    "    with tf.device(\"/GPU:0\"):\n",
    "        product = tf.tensordot(a, b, axes=2)\n",
    "    return product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyTqOmp9UBCO"
   },
   "source": [
    "### Running the benchmark\n",
    "\n",
    "This section declares the start and stop tensor sizes for our test.\n",
    "You can change `SIZE_LIMIT` and then run again; just know that at some point you will run out of memory!\n",
    "\n",
    "Next, it does tests at several sizes within this range, doubling each time.\n",
    "\n",
    "We use [`timeit.timeit()`](https://docs.python.org/3/library/timeit.html#timeit.timeit) for the tests. It will call the function multiple times and then average those times. Timeit is also more accurate than manually calling Python's time function and doing subtraction.\n",
    "\n",
    "Finally, results are saved into a list that's then exported to a pandas DataFrame for easy viewing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from timeit import timeit\n",
    "\n",
    "SIZE_LIMIT: int = 10000  # where to stop at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rsN56pwmJDl3",
    "outputId": "6aa1b237-4d66-4701-fa01-622e015383c6"
   },
   "outputs": [],
   "source": [
    "# This cell is PyTorch\n",
    "tensor_size = 10  # start at size 10\n",
    "torch_results = []\n",
    "\n",
    "print(\"Running PyTorch with 2D tensors from\", tensor_size, \"to\", SIZE_LIMIT, \"square\")\n",
    "\n",
    "# Run the test\n",
    "while tensor_size < SIZE_LIMIT:\n",
    "    # Random array\n",
    "    a = torch.rand(tensor_size, tensor_size, device=\"cpu\")\n",
    "    b = torch.rand(tensor_size, tensor_size, device=\"cpu\")\n",
    "\n",
    "    # Time the CPU operation\n",
    "    cpu_time = timeit(\"torch_cpu_dot_product(a, b)\", globals=globals(), number=50)\n",
    "\n",
    "    # Time the GPU operation\n",
    "    # First, we send the data to the GPU, called the warm up\n",
    "    # It really depends on the application of this time is important or negligible\n",
    "    # We are doing it here becasue timeit() averages the results of multiple runs\n",
    "    torch_gpu_dot_product(a, b)\n",
    "    # Now we time the actual operation\n",
    "    gpu_time = timeit(\"torch_gpu_dot_product(a, b)\", globals=globals(), number=50)\n",
    "\n",
    "    # Record the results\n",
    "    torch_results.append(\n",
    "        {\n",
    "            \"tensor_size\": tensor_size * tensor_size,\n",
    "            \"cpu_time\": cpu_time,\n",
    "            \"gpu_time\": gpu_time,\n",
    "            \"gpu_speedup\": cpu_time / gpu_time,  # Greater than 1 means faster on GPU\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Increase tensor_size by 100. For larger SIZE_LIMITS, change to double tensor_size\n",
    "    # tensor_size = tensor_size * 2\n",
    "    tensor_size = tensor_size + 100\n",
    "\n",
    "# Done! Cast the results to a DataFrame and print\n",
    "torch_results_df = pd.DataFrame(torch_results)\n",
    "print(\"PyTorch Results:\")\n",
    "print(torch_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is TensorFlow\n",
    "tensor_size = 10  # start at size 10\n",
    "tf_results = []\n",
    "\n",
    "print(\n",
    "    \"Running TensorFlow with 2D tensors from\", tensor_size, \"to\", SIZE_LIMIT, \"square\"\n",
    ")\n",
    "\n",
    "# Run the test\n",
    "while tensor_size <= SIZE_LIMIT:\n",
    "    # Random tensor_size x tensor_size array\n",
    "    with tf.device(\"/CPU:0\"):\n",
    "        a = tf.random.uniform((tensor_size, tensor_size))\n",
    "        b = tf.random.uniform((tensor_size, tensor_size))\n",
    "\n",
    "    # Time the CPU operation\n",
    "    cpu_time = timeit(\"tf_cpu_dot_product(a, b)\", globals=globals(), number=10)\n",
    "\n",
    "    # Time the GPU operation\n",
    "    # First, we send the data to the GPU, called the warm up\n",
    "    # It really depends on the application of this time is important or negligible\n",
    "    # We are doing it here because timeit() runs the function multiple times anyway\n",
    "    tf_gpu_dot_product(a, b)\n",
    "    # Now we time the actual operation\n",
    "    gpu_time = timeit(\"tf_gpu_dot_product(a, b)\", globals=globals(), number=10)\n",
    "\n",
    "    # Record the results\n",
    "    tf_results.append(\n",
    "        {\n",
    "            \"tensor_size\": tensor_size * tensor_size,\n",
    "            \"cpu_time\": cpu_time,\n",
    "            \"gpu_time\": gpu_time,\n",
    "            \"gpu_speedup\": cpu_time / gpu_time,  # Greater than 1 means faster on GPU\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Increase tensor_size by 100. For larger SIZE_LIMITS, change to double tensor_size\n",
    "    # tensor_size = tensor_size * 2\n",
    "    tensor_size = tensor_size + 100\n",
    "\n",
    "# Done! Cast the results to a DataFrame and print\n",
    "tf_results_df = pd.DataFrame(tf_results)\n",
    "print(\"TensorFlow Results:\")\n",
    "print(tf_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OXQbzUWUbqo"
   },
   "source": [
    "### Dot Product Results\n",
    "\n",
    "If you left the default sizes, you should see 10 rows of results.\n",
    "You'll notice that with small tensors the CPU is *faster* than the GPU!\n",
    "This is also indidcated by the **gpu_speedup** being less than 1.\n",
    "\n",
    "But as the tensor sizes grow, the GPU overtakes the CPU for speed! 🏎️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VBrJm0YQe7K"
   },
   "source": [
    "## Another Tensor Operation\n",
    "\n",
    "Your task is to repeat this benchmark below, but finding the minimum element in a **1D tensor**.\n",
    "You only need to do it with **one** framework.\n",
    "\n",
    "Use either\n",
    "\n",
    "- [`torch.min()`](https://pytorch.org/docs/stable/generated/torch.min.html) *or*\n",
    "- [`tf.math.reduce_min()](https://www.tensorflow.org/api_docs/python/tf/math/reduce_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0jcwTVFWhey"
   },
   "outputs": [],
   "source": [
    "# Define your methods here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qt8pxSE9WjdS"
   },
   "outputs": [],
   "source": [
    "# Conduct your benchmark here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b0rM3aeWmcD"
   },
   "source": [
    "## Deliverable\n",
    "\n",
    "> **After** answering these questions, download this completed notebook and **upload to Gradescope.**\n",
    "\n",
    "### Reflection 📈\n",
    "\n",
    "### *Why* does the CPU outperform the GPU dot product with smaller vectors?\n",
    "\n",
    "### *How* did the CPU vs. GPU perform for `min()`?\n",
    "\n",
    "#### *Why* did it perform that way?\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
