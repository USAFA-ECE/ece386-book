
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>12. Text Vectorization &#8212; ECE 386</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'llm/text-vectorization';</script>
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="13. Large Language Models" href="llm.html" />
    <link rel="prev" title="11. ICE: Whisper Transcription" href="../b3-devboard/ice-whisper.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="ECE 386 - Home"/>
    <img src="../_static/logo.png" class="logo__image only-dark pst-js-only" alt="ECE 386 - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    AI Hardware Applications
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Prediction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../b1-prediction/hello-colab.html">1. Hello, Colab!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b1-prediction/networks-tooling.html">2. Networks and Tooling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b1-prediction/prediction-machines.html">3. Prediction Machines and ML Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b1-prediction/ice-kmeans.html">4. ICE: K-Means Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b1-prediction/cloud-hosting.html">5. Cloud Hosting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b1-prediction/mnist-dnn.html">6. MNIST Digits with Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b1-prediction/lab-serve-digits.html">7. Lab: Serving Digits with FastAPI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b1-prediction/cc-prediction.html">8. C&amp;C: Prediction and Dimensionality</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Development Boards</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../b3-devboard/lab-cat-dog.html">9. Lab: Cats vs. Dogs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b3-devboard/containerization.html">10. Containerization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../b3-devboard/ice-whisper.html">11. ICE: Whisper Transcription</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Large Language Models</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">12. Text Vectorization</a></li>
<li class="toctree-l1"><a class="reference internal" href="llm.html">13. Large Language Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="prompt-engineering.html">14. Prompt Engineering</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Final Block</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../final/final-project.html">15. Final Project</a></li>




<li class="toctree-l1"><a class="reference internal" href="../final/wrapup.html">20. Wrap and Critique</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/usafa-ece/ai-hardware/blob/main/book/llm/text-vectorization.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/usafa-ece/ai-hardware" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/usafa-ece/ai-hardware/issues/new?title=Issue%20on%20page%20%2Fllm/text-vectorization.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/llm/text-vectorization.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Text Vectorization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-reading">12.1. Pre-reading</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#objectives">Objectives</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">12.2. Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#natural-language-processing">Natural Language Processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#math-with-words">Math with Words</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exploration">Exploration</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standardization">Standardization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">Tokenization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#indexing">Indexing</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise">12.3. Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explore">Explore</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#word-clouds">Word Clouds</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standardize">12.4. Standardize</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stop-words">Stop Words</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stemming">Stemming</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#lemmatization">Lemmatization</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenize">12.5. Tokenize</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#index">12.6. Index</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bag-of-words">Bag of Words</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bigrams">Bigrams</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">12.7. Conclusion</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="text-vectorization">
<h1><span class="section-number">12. </span>Text Vectorization<a class="headerlink" href="#text-vectorization" title="Link to this heading">#</a></h1>
<p>The first step in Natural Language Processing (NLP) is to get the words into a format that we can do math on them.</p>
<section id="pre-reading">
<h2><span class="section-number">12.1. </span>Pre-reading<a class="headerlink" href="#pre-reading" title="Link to this heading">#</a></h2>
<p>Lightly read this:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.deeplearning.ai/resources/natural-language-processing/">DeepLearning AI: A Complete Guide to Natural Language Processing</a></p></li>
</ul>
<p>Be prepared to reference this:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://learning.oreilly.com/library/view/deep-learning-with/9781617296864/Text/11.htm"><em>Deep Learning with Python</em>, 11.0 - 11.3</a></p></li>
</ul>
<section id="objectives">
<h3>Objectives<a class="headerlink" href="#objectives" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Gain a basic understanding of natural language processing (NLP)</p></li>
<li><p>Prepare text data for computer processing.</p></li>
<li><p>Vectorize text.</p></li>
</ul>
</section>
</section>
<section id="overview">
<h2><span class="section-number">12.2. </span>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<section id="natural-language-processing">
<h3>Natural Language Processing<a class="headerlink" href="#natural-language-processing" title="Link to this heading">#</a></h3>
<blockquote>
<div><p>Natural language processing (NLP) is a field of computer science and a subfield of artificial intelligence that aims to make computers understand human language. NLP uses computational linguistics, which is the study of how language works, and various models based on statistics, machine learning, and deep learning. ~ Geeks for Geeks</p>
</div></blockquote>
<p><em>See the DeepLearning AI post for more <strong>why</strong>, what, and how</em>.</p>
</section>
<section id="math-with-words">
<h3>Math with Words<a class="headerlink" href="#math-with-words" title="Link to this heading">#</a></h3>
<blockquote>
<div><p>Deep learning models, being differentiable functions, can only process numeric tensors: they can’t take raw text as input. <em>Vectorizing</em> text is the process of transforming text into numeric tensors.
~ <em>Deep Learning with Python</em></p>
</div></blockquote>
<ol class="arabic simple" start="0">
<li><p><strong>Explore</strong> the dataset to see understand what it contains.</p></li>
<li><p><strong>Standardize</strong> text to make it easier to process, such as by converting it to lowercase or removing formatting.</p></li>
<li><p><strong>Tokenize</strong> the text by splitting it into units.</p></li>
<li><p><strong>Index</strong> the tokens into a numerical vector.</p></li>
</ol>
<p><img alt="From raw text to vectors, Deep Learning with Python, 2nd Ed, fig. 11.1" src="../_images/deep_learning_with_python-fig-11-01.png" /></p>
<section id="exploration">
<h4>Exploration<a class="headerlink" href="#exploration" title="Link to this heading">#</a></h4>
<p>Although not listed in the text book, but you should always begin with exploring the dataset to understand what it contains: data format and potential bias!</p>
</section>
</section>
<section id="standardization">
<h3>Standardization<a class="headerlink" href="#standardization" title="Link to this heading">#</a></h3>
<p>Examples of standardization include converting to lowercase, standardizing punctuation and special characters, and stemming.</p>
<pre  class="mermaid">
        graph LR
   A[&quot;My altitude is 7258&quot; above sea-level, far, far above that of West Point or Annapolis!&quot;] --&gt; norm((&quot;Normalize Text&quot;))
   B[&quot;My altitude is 7258 ft. above sea level, FAR FAR above that of west point or Annapolis!&quot;] --&gt; norm
   norm --&gt; result[&quot;my altitude is 7258 feet above sea level far far above that of west point or annapolis !&quot;]
    </pre></section>
<section id="tokenization">
<h3>Tokenization<a class="headerlink" href="#tokenization" title="Link to this heading">#</a></h3>
<p>You can tokenize in different ways.</p>
<p>Here is an example of <strong>word-level</strong> tokenization.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;my&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;altitude&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;is&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;7258&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;feet&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;above&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;sea&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;level&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;far&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;far&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;above&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;that&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;of&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;west&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;point&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;or&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;annapolis&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;!&quot;</span><span class="p">}</span>
</pre></div>
</div>
<p>Here is an example of <strong>bag-of-3-grams</strong> tokenization.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;my altitude is&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;altitude is 7258&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;is 7258 feet&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;7258 feet above&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;feet above sea&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;above sea level&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;sea level far&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;level far far&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;far far above&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;far above that&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;above that of&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;that of west&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;of west point&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;west point or&quot;</span><span class="p">,</span><span class="w"> </span><span class="s2">&quot;point or annapolis&quot;</span><span class="p">}</span>
</pre></div>
</div>
</section>
<section id="indexing">
<h3>Indexing<a class="headerlink" href="#indexing" title="Link to this heading">#</a></h3>
<p>The simplest way to represent tokens in a vector is with the <strong>bag-of-words</strong> approach, which just counts how many times each token appears in the text.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="nt">&quot;my&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;altitude&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;is&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;7258&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;feet&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;above&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;sea&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;level&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;far&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;that&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;of&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;west&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;point&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;or&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;annapolis&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nt">&quot;!&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">}</span>
</pre></div>
</div>
<p>As simple as this is, it can be highly effective! However, you lose sequence information, which can be critical. Moving to N-grams can help!</p>
<p><strong>Sequence models</strong> are a more advanced method of retaining sequence information, for more advanced use-cases.</p>
</section>
</section>
<section id="exercise">
<h2><span class="section-number">12.3. </span>Exercise<a class="headerlink" href="#exercise" title="Link to this heading">#</a></h2>
<p>For this exercise we will use <a class="reference external" href="https://www.presidency.ucsb.edu/documents/app-categories/spoken-addresses-and-remarks/presidential/inaugural-addresses">Inaugural Addresses from American Presidents</a>.</p>
<p>Go to the website now and think how you might put all of these into an easy-to-ingest document.</p>
<p>Fortunately, I”ve already extracted some of these and placed them in <a class="reference download internal" download="" href="../_downloads/87beeb0ef3dbf9c0885c085903f931e5/inaugural_addresses.csv"><span class="xref download myst">book/data/inagural_addresses.csv</span></a></p>
<section id="explore">
<h3>Explore<a class="headerlink" href="#explore" title="Link to this heading">#</a></h3>
<p>As always, we should preview some stats about what we are diving in to.</p>
<blockquote>
<div><p><strong>Prompt GPT4-Advanced Data Analytics</strong>: Use pandas to provide a quick summary of this CSV</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download the dataset, if not running in VSCode</span>
<span class="c1"># !wget https://raw.githubusercontent.com/USAFA-ECE/ece386-book/refs/heads/main/book/data/inaugural_addresses.csv</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Change if running in colab</span>
<span class="n">csv_path</span> <span class="o">=</span> <span class="s2">&quot;../data/inaugural_addresses.csv&quot;</span>

<span class="c1"># Load the CSV into a pandas DataFrame</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csv_path</span><span class="p">)</span>

<span class="c1"># Display the first few rows of the DataFrame and its summary</span>
<span class="n">df_head</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
<span class="n">df_info</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>

<span class="n">df_head</span>
</pre></div>
</div>
</div>
</div>
<section id="word-clouds">
<h4>Word Clouds<a class="headerlink" href="#word-clouds" title="Link to this heading">#</a></h4>
<p>Unlike numerical data, we cannot easily do things like mean, median, or standard deviation with text data.</p>
<p>Let”s try a word cloud, just for fun.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">pip</span> install -q wordcloud
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">wordcloud</span> <span class="kn">import</span> <span class="n">WordCloud</span>


<span class="k">def</span> <span class="nf">plot_wordcloud</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">column</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;Text&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># Set up the figure size and number of subplots</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">30</span><span class="p">))</span>

    <span class="c1"># Loop through each row of the DataFrame and generate a word cloud from the column</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">row</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">()):</span>
        <span class="c1"># Create a word cloud object</span>
        <span class="n">wc</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">(</span>
            <span class="c1"># stopwords is empty here, but can replace with wordcloud.STOPWORDS as a default list</span>
            <span class="n">background_color</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">,</span>
            <span class="n">stopwords</span><span class="o">=</span><span class="p">[],</span>
            <span class="n">max_words</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
            <span class="n">width</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span>
            <span class="n">height</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Generate the word cloud from the column variable</span>
        <span class="n">wc</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="n">column</span><span class="p">])</span>

        <span class="c1"># Display the word cloud on the subplot</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">wc</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s2">&quot;bilinear&quot;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;President&quot;</span><span class="p">]</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;Year&quot;</span><span class="p">]</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">37</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_wordcloud</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="standardize">
<h2><span class="section-number">12.4. </span>Standardize<a class="headerlink" href="#standardize" title="Link to this heading">#</a></h2>
<p>We will do the following to standardize our dataset:</p>
<ol class="arabic simple">
<li><p>Convert to lowercase</p></li>
<li><p>Remove stop words</p></li>
<li><p>Apply stemming</p></li>
</ol>
<section id="stop-words">
<h3>Stop Words<a class="headerlink" href="#stop-words" title="Link to this heading">#</a></h3>
<p>As you can see in word clouds, words such as “and” and “the” dominate, but don”t provide very much meaning.</p>
<p>To combat this, we will be <a class="reference external" href="https://www.geeksforgeeks.org/removing-stop-words-nltk-python/">Removing stop words with NLTK in Python</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>By default the <code class="docutils literal notranslate"><span class="pre">WordCloud</span></code> class applies english stop words present in the wordcloud.STOPWORDS list.
The code above deliberately prevented this by passing the argument <code class="docutils literal notranslate"><span class="pre">stopwords=[]</span></code>.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">pip</span> install -q nltk
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>

<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s2">&quot;stopwords&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="stemming">
<h3>Stemming<a class="headerlink" href="#stemming" title="Link to this heading">#</a></h3>
<p><strong>Stemming</strong> reduces an inflected word to its base; for example: runs; running; ran –&gt; “run”.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">PorterStemmer</span>

<span class="c1"># create an object of class PorterStemmer</span>
<span class="n">porter</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">porter</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="s2">&quot;play&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">porter</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="s2">&quot;playing&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">porter</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="s2">&quot;plays&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">porter</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="s2">&quot;played&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<section id="lemmatization">
<h4>Lemmatization<a class="headerlink" href="#lemmatization" title="Link to this heading">#</a></h4>
<p>Another common text pre-processing technique is <a class="reference external" href="https://en.wikipedia.org/wiki/Lemmatization">lemmatization</a>.</p>
<blockquote>
<div><p>In linguistics, is the process of grouping together the inflected forms of a word so they can be analyzed as a single item, identified by the word”s lemma, or dictionary form.</p>
</div></blockquote>
<p><strong>Stemming</strong> reduces an inflected word to its base; for example: runs; running; ran –&gt; “run”.</p>
<p><strong>Lemmatizing</strong> goes further by using knowledge of surrounding words.</p>
<ol class="arabic simple">
<li><p>The word “better” has “good” as its lemma. This link is missed by stemming, as it requires a dictionary look-up.</p></li>
<li><p>The word “walk” is the base form for the word “walking”, and hence this is matched in both stemming and lemmatization.</p></li>
<li><p>The word “meeting” can be either the base form of a noun or a form of a verb (“to meet”) depending on the context; e.g., “in our last meeting” or “We are meeting again tomorrow”. Unlike stemming, lemmatization attempts to select the correct lemma depending on the context.</p></li>
</ol>
</section>
</section>
</section>
<section id="tokenize">
<h2><span class="section-number">12.5. </span>Tokenize<a class="headerlink" href="#tokenize" title="Link to this heading">#</a></h2>
<p>Because of how <code class="docutils literal notranslate"><span class="pre">nltk</span></code> works, we will actually standardize while we tokenize. In our case, we will just do <strong>word</strong> tokens, but there are <em>many</em> other options!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">PorterStemmer</span>

<span class="c1"># Assume we previously loaded inaugural_addresses.csv into df</span>

<span class="c1"># Initialize the stemmer</span>
<span class="n">stemmer</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>


<span class="c1"># Define a function that applies stemming and stopwords removal</span>
<span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># Tokenize the text word-by-word</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

    <span class="c1"># Convert to lowercase, remove stopwords, and apply stemming</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tokens</span>
        <span class="k">if</span> <span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="k">return</span> <span class="n">tokens</span>


<span class="c1"># Apply the function to the &quot;text&quot; column</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;tokens&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;Text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">preprocess</span><span class="p">)</span>

<span class="c1"># Preview the result</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Original text: </span><span class="se">\n</span><span class="si">{</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;Text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tokens: </span><span class="se">\n</span><span class="si">{</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;tokens&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Put clean text back into a string for wordcloud</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;standardized_text&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;tokens&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plot_wordcloud</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s2">&quot;standardized_text&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="index">
<h2><span class="section-number">12.6. </span>Index<a class="headerlink" href="#index" title="Link to this heading">#</a></h2>
<p>Now we get to put our standardized words into a vector!</p>
<p>We will be using scikit-learn”s <a class="reference external" href="https://www.geeksforgeeks.org/using-countvectorizer-to-extracting-features-from-text/">CountVectorizer to Extracting Features from Text (Geeks for Geeks)</a>.</p>
<blockquote>
<div><p>Class <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"><code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code></a> converts a collection of text documents to a matrix of token counts.
This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.</p>
</div></blockquote>
<section id="bag-of-words">
<h3>Bag of Words<a class="headerlink" href="#bag-of-words" title="Link to this heading">#</a></h3>
<p>The naive - but sometimes highly effective - approach is the “Bag of Words” approach: simply count how many times words show up!</p>
<p>This is actually what are word clouds are doing under the hood!</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>This produces a <strong>sparse matrix</strong>, meaning there are lots of zeros!
As a pro, such matrices can be highly compressed. However, they also present unique challenges in machine learning.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="c1"># Create a Vectorizer Object</span>
<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>

<span class="n">document</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;standardized_text&quot;</span><span class="p">]</span>

<span class="n">vectorizer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">document</span><span class="p">)</span>

<span class="c1"># Printing the identified Unique words along with their indices</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vocabulary: &quot;</span><span class="p">,</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">)</span>

<span class="c1"># Encode the Document</span>
<span class="n">vector</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">document</span><span class="p">)</span>

<span class="c1"># Summarizing the Encoded Texts</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Encoded Document is:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vector</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<section id="bigrams">
<h4>Bigrams<a class="headerlink" href="#bigrams" title="Link to this heading">#</a></h4>
<p>We could instead <a class="reference external" href="https://www.geeksforgeeks.org/generate-bigrams-with-nltk/">generate bigrams with NLTK (Geeks for Geeks)</a>, and then index these. This could further increase our accuracy for some applications, but is more complex.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nltk.util</span> <span class="kn">import</span> <span class="n">bigrams</span>

<span class="n">bigram_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">bigrams</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;tokens&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bigrams for the first document:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">bigram</span> <span class="ow">in</span> <span class="n">bigram_list</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">bigram</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="conclusion">
<h2><span class="section-number">12.7. </span>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>In this exercise you”ve learned some basics of how to explore, standardize, tokenize, and index words! This is critical to understand how NLP (including Large Language Models) is possible!</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./llm"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../b3-devboard/ice-whisper.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">11. </span>ICE: Whisper Transcription</p>
      </div>
    </a>
    <a class="right-next"
       href="llm.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">13. </span>Large Language Models</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-reading">12.1. Pre-reading</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#objectives">Objectives</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">12.2. Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#natural-language-processing">Natural Language Processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#math-with-words">Math with Words</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exploration">Exploration</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standardization">Standardization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">Tokenization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#indexing">Indexing</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise">12.3. Exercise</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explore">Explore</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#word-clouds">Word Clouds</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standardize">12.4. Standardize</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stop-words">Stop Words</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stemming">Stemming</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#lemmatization">Lemmatization</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenize">12.5. Tokenize</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#index">12.6. Index</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bag-of-words">Bag of Words</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bigrams">Bigrams</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">12.7. Conclusion</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By DFEC
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>